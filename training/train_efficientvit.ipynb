{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyEfficientViT Segmentation Training\n",
        "\n",
        "This notebook trains a TinyEfficientViT model for eye/pupil segmentation on the OpenEDS dataset.\n",
        "\n",
        "**Features:**\n",
        "- Free GPU access via Google Colab\n",
        "- Interactive experimentation and visualization\n",
        "- No external infrastructure required (Modal, MLflow, etc.)\n",
        "\n",
        "**Model Constraints:**\n",
        "- Target: <60k parameters for edge deployment\n",
        "- Input: 640x400 grayscale images\n",
        "- Output: 2-class segmentation (background, pupil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected!\")\n",
        "    print(\"Go to Runtime -> Change runtime type -> GPU\")\n",
        "    raise RuntimeError(\"GPU required for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision opencv-python-headless Pillow scikit-learn tqdm matplotlib datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load OpenEDS dataset from HuggingFace\n",
        "HF_DATASET_REPO = \"Conner/openeds-precomputed\"\n",
        "\n",
        "print(f\"Loading dataset from HuggingFace: {HF_DATASET_REPO}\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "hf_dataset = load_dataset(HF_DATASET_REPO)\n",
        "\n",
        "print(f\"\\nDataset loaded!\")\n",
        "print(f\"Train samples: {len(hf_dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(hf_dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample images from dataset\n",
        "IMAGE_HEIGHT = 400\n",
        "IMAGE_WIDTH = 640\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "for i in range(2):\n",
        "    sample = hf_dataset['train'][i * 100]\n",
        "    image = np.array(sample['image'], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    label = np.array(sample['label'], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    \n",
        "    axes[i, 0].imshow(image, cmap='gray')\n",
        "    axes[i, 0].set_title(f\"Image {i+1}\")\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    axes[i, 1].imshow(label, cmap='jet', vmin=0, vmax=1)\n",
        "    axes[i, 1].set_title(f\"Label {i+1}\")\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    # Overlay\n",
        "    overlay = image.copy()\n",
        "    overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
        "    overlay[label == 1] = [255, 0, 0]  # Pupil in red\n",
        "    axes[i, 2].imshow(overlay)\n",
        "    axes[i, 2].set_title(f\"Overlay {i+1}\")\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture: TinyEfficientViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyConvNorm(nn.Module):\n",
        "    \"\"\"Convolution + BatchNorm layer (parameter-efficient).\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1, bias=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "\n",
        "class TinyPatchEmbedding(nn.Module):\n",
        "    \"\"\"Lightweight patch embedding with 2 conv layers and stride 4.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, embed_dim=8):\n",
        "        super().__init__()\n",
        "        mid_dim = embed_dim // 2 if embed_dim >= 4 else 2\n",
        "        self.conv1 = TinyConvNorm(in_channels, mid_dim, kernel_size=3, stride=2, padding=1)\n",
        "        self.act1 = nn.GELU()\n",
        "        self.conv2 = TinyConvNorm(mid_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n",
        "        self.act2 = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyCascadedGroupAttention(nn.Module):\n",
        "    \"\"\"Tiny version of Cascaded Group Attention.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=1, key_dim=4, attn_ratio=2):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.scale = key_dim**-0.5\n",
        "        self.d = int(attn_ratio * key_dim)\n",
        "        self.attn_ratio = attn_ratio\n",
        "\n",
        "        qkv_dim = (num_heads * key_dim * 2) + (num_heads * self.d)\n",
        "        self.qkv = nn.Linear(dim, qkv_dim)\n",
        "        self.proj = nn.Linear(num_heads * self.d, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "\n",
        "        q_total = self.num_heads * self.key_dim\n",
        "        k_total = self.num_heads * self.key_dim\n",
        "\n",
        "        q = qkv[:, :, :q_total].reshape(B, N, self.num_heads, self.key_dim).permute(0, 2, 1, 3)\n",
        "        k = qkv[:, :, q_total:q_total + k_total].reshape(B, N, self.num_heads, self.key_dim).permute(0, 2, 1, 3)\n",
        "        v = qkv[:, :, q_total + k_total:].reshape(B, N, self.num_heads, self.d).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.num_heads * self.d)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyLocalWindowAttention(nn.Module):\n",
        "    \"\"\"Local window attention wrapper.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=1, key_dim=4, attn_ratio=2, window_size=7):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.attn = TinyCascadedGroupAttention(dim=dim, num_heads=num_heads, key_dim=key_dim, attn_ratio=attn_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        ws = self.window_size\n",
        "\n",
        "        pad_h = (ws - H % ws) % ws\n",
        "        pad_w = (ws - W % ws) % ws\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "        _, _, Hp, Wp = x.shape\n",
        "\n",
        "        x = x.view(B, C, Hp // ws, ws, Wp // ws, ws)\n",
        "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
        "        x = x.view(B * (Hp // ws) * (Wp // ws), ws * ws, C)\n",
        "\n",
        "        x = self.attn(x)\n",
        "\n",
        "        x = x.view(B, Hp // ws, Wp // ws, ws, ws, C)\n",
        "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
        "        x = x.view(B, C, Hp, Wp)\n",
        "\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = x[:, :, :H, :W]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyMLP(nn.Module):\n",
        "    \"\"\"Tiny MLP with expansion ratio.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, expansion_ratio=2):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(dim * expansion_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "\n",
        "class TinyEfficientVitBlock(nn.Module):\n",
        "    \"\"\"Single EfficientViT block.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=1, key_dim=4, attn_ratio=2, window_size=7, mlp_ratio=2):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(dim)\n",
        "        self.dw_conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim)\n",
        "        self.norm2 = nn.BatchNorm2d(dim)\n",
        "        self.attn = TinyLocalWindowAttention(dim=dim, num_heads=num_heads, key_dim=key_dim, attn_ratio=attn_ratio, window_size=window_size)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.mlp = TinyMLP(dim, expansion_ratio=mlp_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dw_conv(self.norm1(x))\n",
        "        x = x + self.attn(self.norm2(x))\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
        "        x_flat = x_flat + self.mlp(self.norm3(x_flat))\n",
        "        x = x_flat.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyEfficientVitStage(nn.Module):\n",
        "    \"\"\"Single stage of TinyEfficientViT.\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, depth=1, num_heads=1, key_dim=4, attn_ratio=2, window_size=7, mlp_ratio=2, downsample=True):\n",
        "        super().__init__()\n",
        "        self.downsample = None\n",
        "        if downsample:\n",
        "            self.downsample = nn.Sequential(\n",
        "                TinyConvNorm(in_dim, out_dim, kernel_size=3, stride=2, padding=1),\n",
        "                nn.GELU(),\n",
        "            )\n",
        "        elif in_dim != out_dim:\n",
        "            self.downsample = nn.Sequential(\n",
        "                TinyConvNorm(in_dim, out_dim, kernel_size=1, stride=1, padding=0),\n",
        "                nn.GELU(),\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TinyEfficientVitBlock(dim=out_dim, num_heads=num_heads, key_dim=key_dim, attn_ratio=attn_ratio, window_size=window_size, mlp_ratio=mlp_ratio)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyEfficientVitEncoder(nn.Module):\n",
        "    \"\"\"Complete TinyEfficientViT encoder with 3 stages.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, embed_dims=(8, 16, 24), depths=(1, 1, 1), num_heads=(1, 1, 2), key_dims=(4, 4, 4), attn_ratios=(2, 2, 2), window_sizes=(7, 7, 7), mlp_ratios=(2, 2, 2)):\n",
        "        super().__init__()\n",
        "        self.patch_embed = TinyPatchEmbedding(in_channels=in_channels, embed_dim=embed_dims[0])\n",
        "\n",
        "        self.stage1 = TinyEfficientVitStage(in_dim=embed_dims[0], out_dim=embed_dims[0], depth=depths[0], num_heads=num_heads[0], key_dim=key_dims[0], attn_ratio=attn_ratios[0], window_size=window_sizes[0], mlp_ratio=mlp_ratios[0], downsample=False)\n",
        "        self.stage2 = TinyEfficientVitStage(in_dim=embed_dims[0], out_dim=embed_dims[1], depth=depths[1], num_heads=num_heads[1], key_dim=key_dims[1], attn_ratio=attn_ratios[1], window_size=window_sizes[1], mlp_ratio=mlp_ratios[1], downsample=True)\n",
        "        self.stage3 = TinyEfficientVitStage(in_dim=embed_dims[1], out_dim=embed_dims[2], depth=depths[2], num_heads=num_heads[2], key_dim=key_dims[2], attn_ratio=attn_ratios[2], window_size=window_sizes[2], mlp_ratio=mlp_ratios[2], downsample=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        f1 = self.stage1(x)\n",
        "        f2 = self.stage2(f1)\n",
        "        f3 = self.stage3(f2)\n",
        "        return f1, f2, f3\n",
        "\n",
        "\n",
        "class TinySegmentationDecoder(nn.Module):\n",
        "    \"\"\"Lightweight FPN-style decoder with skip connections.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dims=(8, 16, 24), decoder_dim=16, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lateral3 = nn.Conv2d(encoder_dims[2], decoder_dim, kernel_size=1)\n",
        "        self.lateral2 = nn.Conv2d(encoder_dims[1], decoder_dim, kernel_size=1)\n",
        "        self.lateral1 = nn.Conv2d(encoder_dims[0], decoder_dim, kernel_size=1)\n",
        "\n",
        "        self.smooth3 = nn.Sequential(\n",
        "            nn.Conv2d(decoder_dim, decoder_dim, kernel_size=3, padding=1, groups=decoder_dim),\n",
        "            nn.BatchNorm2d(decoder_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.smooth2 = nn.Sequential(\n",
        "            nn.Conv2d(decoder_dim, decoder_dim, kernel_size=3, padding=1, groups=decoder_dim),\n",
        "            nn.BatchNorm2d(decoder_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.smooth1 = nn.Sequential(\n",
        "            nn.Conv2d(decoder_dim, decoder_dim, kernel_size=3, padding=1, groups=decoder_dim),\n",
        "            nn.BatchNorm2d(decoder_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.head = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, f1, f2, f3, target_size):\n",
        "        p3 = self.lateral3(f3)\n",
        "        p3 = self.smooth3(p3)\n",
        "\n",
        "        p2 = self.lateral2(f2) + F.interpolate(p3, size=f2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        p2 = self.smooth2(p2)\n",
        "\n",
        "        p1 = self.lateral1(f1) + F.interpolate(p2, size=f1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        p1 = self.smooth1(p1)\n",
        "\n",
        "        out = self.head(p1)\n",
        "        out = F.interpolate(out, size=target_size, mode=\"bilinear\", align_corners=False)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TinyEfficientViTSeg(nn.Module):\n",
        "    \"\"\"Complete TinyEfficientViT for semantic segmentation (<60k parameters).\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, num_classes=2, embed_dims=(8, 16, 24), depths=(1, 1, 1), num_heads=(1, 1, 2), key_dims=(4, 4, 4), attn_ratios=(2, 2, 2), window_sizes=(7, 7, 7), mlp_ratios=(2, 2, 2), decoder_dim=16):\n",
        "        super().__init__()\n",
        "        self.encoder = TinyEfficientVitEncoder(in_channels=in_channels, embed_dims=embed_dims, depths=depths, num_heads=num_heads, key_dims=key_dims, attn_ratios=attn_ratios, window_sizes=window_sizes, mlp_ratios=mlp_ratios)\n",
        "        self.decoder = TinySegmentationDecoder(encoder_dims=embed_dims, decoder_dim=decoder_dim, num_classes=num_classes)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        target_size = (x.shape[2], x.shape[3])\n",
        "        f1, f2, f3 = self.encoder(x)\n",
        "        out = self.decoder(f1, f2, f3, target_size)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Loss Functions and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined Cross-Entropy + Dice + Surface Loss.\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.nll = nn.NLLLoss(reduction=\"none\")\n",
        "\n",
        "    def forward(self, logits, target, spatial_weights, dist_map, alpha):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        ce_loss = self.nll(log_probs, target)\n",
        "        weighted_ce = (ce_loss * (1.0 + spatial_weights)).mean()\n",
        "\n",
        "        target_onehot = F.one_hot(target, num_classes=2).permute(0, 3, 1, 2).float()\n",
        "        probs_flat = probs.flatten(start_dim=2)\n",
        "        target_flat = target_onehot.flatten(start_dim=2)\n",
        "        intersection = (probs_flat * target_flat).sum(dim=2)\n",
        "        cardinality = (probs_flat + target_flat).sum(dim=2)\n",
        "        class_weights = 1.0 / (target_flat.sum(dim=2) ** 2).clamp(min=self.epsilon)\n",
        "        dice = 2.0 * (class_weights * intersection).sum(dim=1) / (class_weights * cardinality).sum(dim=1)\n",
        "        dice_loss = (1.0 - dice.clamp(min=self.epsilon)).mean()\n",
        "\n",
        "        surface_loss = (probs.flatten(start_dim=2) * dist_map.flatten(start_dim=2)).mean(dim=2).mean(dim=1).mean()\n",
        "\n",
        "        total_loss = weighted_ce + alpha * dice_loss + (1.0 - alpha) * surface_loss\n",
        "        return total_loss, weighted_ce, dice_loss, surface_loss\n",
        "\n",
        "\n",
        "def compute_iou_tensors(predictions, targets, num_classes=2):\n",
        "    \"\"\"Compute IoU for each class.\"\"\"\n",
        "    intersection = torch.zeros(num_classes, device=predictions.device)\n",
        "    union = torch.zeros(num_classes, device=predictions.device)\n",
        "    for c in range(num_classes):\n",
        "        pred_c = predictions == c\n",
        "        target_c = targets == c\n",
        "        intersection[c] = torch.logical_and(pred_c, target_c).sum().float()\n",
        "        union[c] = torch.logical_or(pred_c, target_c).sum().float()\n",
        "    return intersection, union\n",
        "\n",
        "\n",
        "def finalize_iou(total_intersection, total_union):\n",
        "    \"\"\"Compute final mIoU from accumulated intersection/union.\"\"\"\n",
        "    iou_per_class = (total_intersection / total_union.clamp(min=1)).cpu().numpy()\n",
        "    return float(np.mean(iou_per_class)), iou_per_class.tolist()\n",
        "\n",
        "\n",
        "def get_predictions(output):\n",
        "    \"\"\"Get class predictions from model output.\"\"\"\n",
        "    bs, _, h, w = output.size()\n",
        "    _, indices = output.max(1)\n",
        "    indices = indices.view(bs, h, w)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def get_nparams(model):\n",
        "    \"\"\"Count trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Augmentation and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomHorizontalFlip:\n",
        "    def __call__(self, img, label):\n",
        "        if random.random() < 0.5:\n",
        "            return img.transpose(Image.FLIP_LEFT_RIGHT), label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "class Gaussian_blur:\n",
        "    def __call__(self, img):\n",
        "        sigma_value = np.random.randint(2, 7)\n",
        "        return Image.fromarray(cv2.GaussianBlur(img, (7, 7), sigma_value))\n",
        "\n",
        "\n",
        "class Line_augment:\n",
        "    def __call__(self, base):\n",
        "        yc, xc = (0.3 + 0.4 * np.random.rand(1)) * np.array(base.shape)\n",
        "        aug_base = np.copy(base)\n",
        "        num_lines = np.random.randint(1, 10)\n",
        "        for _ in np.arange(0, num_lines):\n",
        "            theta = np.pi * np.random.rand(1)\n",
        "            x1 = xc - 50 * np.random.rand(1) * (1 if np.random.rand(1) < 0.5 else -1)\n",
        "            y1 = (x1 - xc) * np.tan(theta) + yc\n",
        "            x2 = xc - (150 * np.random.rand(1) + 50) * (1 if np.random.rand(1) < 0.5 else -1)\n",
        "            y2 = (x2 - xc) * np.tan(theta) + yc\n",
        "            aug_base = cv2.line(aug_base, (int(x1), int(y1)), (int(x2), int(y2)), (255, 255, 255), 4)\n",
        "        aug_base = aug_base.astype(np.uint8)\n",
        "        return Image.fromarray(aug_base)\n",
        "\n",
        "\n",
        "class MaskToTensor:\n",
        "    def __call__(self, img):\n",
        "        return torch.from_numpy(np.array(img, dtype=np.int64)).long()\n",
        "\n",
        "\n",
        "class IrisDataset(Dataset):\n",
        "    \"\"\"Dataset for OpenEDS iris/pupil segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, hf_dataset, split=\"train\", transform=None):\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n",
        "        self.gamma_table = 255.0 * (np.linspace(0, 1, 256) ** 0.8)\n",
        "        self.dataset = hf_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        image = np.array(sample[\"image\"], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "        label = np.array(sample[\"label\"], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "        spatial_weights = np.array(sample[\"spatial_weights\"], dtype=np.float32).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "        dist_map = np.array(sample[\"dist_map\"], dtype=np.float32).reshape(2, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "        filename = sample[\"filename\"]\n",
        "\n",
        "        pilimg = cv2.LUT(image, self.gamma_table)\n",
        "        if self.transform is not None and self.split == \"train\":\n",
        "            if random.random() < 0.2:\n",
        "                pilimg = Line_augment()(np.array(pilimg))\n",
        "            if random.random() < 0.2:\n",
        "                pilimg = Gaussian_blur()(np.array(pilimg))\n",
        "        img = self.clahe.apply(np.array(np.uint8(pilimg)))\n",
        "        img = Image.fromarray(img)\n",
        "        label_pil = Image.fromarray(label)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            if self.split == \"train\":\n",
        "                img, label_pil = RandomHorizontalFlip()(img, label_pil)\n",
        "                if np.array(label_pil)[0, 0] != label[0, 0]:\n",
        "                    spatial_weights = np.fliplr(spatial_weights).copy()\n",
        "                    dist_map = np.flip(dist_map, axis=2).copy()\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label_tensor = MaskToTensor()(label_pil)\n",
        "        return img, label_tensor, filename, spatial_weights, dist_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_curves(train_metrics, valid_metrics):\n",
        "    \"\"\"Plot training and validation curves.\"\"\"\n",
        "    epochs = range(1, len(train_metrics[\"loss\"]) + 1)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 0].plot(epochs, train_metrics[\"loss\"], \"b-\", label=\"Train\", linewidth=2)\n",
        "    axes[0, 0].plot(epochs, valid_metrics[\"loss\"], \"r-\", label=\"Valid\", linewidth=2)\n",
        "    axes[0, 0].set_xlabel(\"Epoch\")\n",
        "    axes[0, 0].set_ylabel(\"Loss\")\n",
        "    axes[0, 0].set_title(\"Training and Validation Loss\", fontweight=\"bold\")\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # mIoU\n",
        "    axes[0, 1].plot(epochs, train_metrics[\"iou\"], \"b-\", label=\"Train\", linewidth=2)\n",
        "    axes[0, 1].plot(epochs, valid_metrics[\"iou\"], \"r-\", label=\"Valid\", linewidth=2)\n",
        "    axes[0, 1].set_xlabel(\"Epoch\")\n",
        "    axes[0, 1].set_ylabel(\"mIoU\")\n",
        "    axes[0, 1].set_title(\"Training and Validation mIoU\", fontweight=\"bold\")\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    if \"lr\" in train_metrics:\n",
        "        axes[1, 0].plot(epochs, train_metrics[\"lr\"], \"g-\", linewidth=2)\n",
        "        axes[1, 0].set_xlabel(\"Epoch\")\n",
        "        axes[1, 0].set_ylabel(\"Learning Rate\")\n",
        "        axes[1, 0].set_title(\"Learning Rate Schedule\", fontweight=\"bold\")\n",
        "        axes[1, 0].set_yscale(\"log\")\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Per-class IoU\n",
        "    if \"background_iou\" in valid_metrics:\n",
        "        axes[1, 1].plot(epochs, valid_metrics[\"background_iou\"], \"b-\", label=\"Background\", linewidth=2)\n",
        "        axes[1, 1].plot(epochs, valid_metrics[\"pupil_iou\"], \"r-\", label=\"Pupil\", linewidth=2)\n",
        "        axes[1, 1].set_xlabel(\"Epoch\")\n",
        "        axes[1, 1].set_ylabel(\"IoU\")\n",
        "        axes[1, 1].set_title(\"Per-Class IoU (Validation)\", fontweight=\"bold\")\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, num_samples=4):\n",
        "    \"\"\"Visualize model predictions.\"\"\"\n",
        "    model.eval()\n",
        "    samples_collected = 0\n",
        "    images_to_plot = []\n",
        "    labels_to_plot = []\n",
        "    preds_to_plot = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img, labels, _, _, _ in dataloader:\n",
        "            if samples_collected >= num_samples:\n",
        "                break\n",
        "            single_img = img[0:1].to(device, memory_format=torch.channels_last)\n",
        "            single_target = labels[0:1].to(device).long()\n",
        "            output = model(single_img)\n",
        "            predictions = get_predictions(output)\n",
        "            images_to_plot.append(img[0].cpu().squeeze().numpy())\n",
        "            labels_to_plot.append(single_target[0].cpu().numpy())\n",
        "            preds_to_plot.append(predictions[0].cpu().numpy())\n",
        "            samples_collected += 1\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i, 0].imshow(images_to_plot[i], cmap=\"gray\")\n",
        "        axes[i, 0].set_title(\"Input Image\", fontweight=\"bold\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "        axes[i, 1].imshow(labels_to_plot[i], cmap=\"jet\", vmin=0, vmax=1)\n",
        "        axes[i, 1].set_title(\"Ground Truth\", fontweight=\"bold\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "        axes[i, 2].imshow(preds_to_plot[i], cmap=\"jet\", vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(\"Prediction\", fontweight=\"bold\")\n",
        "        axes[i, 2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32  # Reduced for Colab memory constraints\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-2\n",
        "NUM_WORKERS = 2  # Colab has limited workers\n",
        "\n",
        "# Model configuration\n",
        "model = TinyEfficientViTSeg(\n",
        "    in_channels=1,\n",
        "    num_classes=2,\n",
        "    embed_dims=(16, 32, 64),\n",
        "    depths=(1, 1, 1),\n",
        "    num_heads=(1, 1, 2),\n",
        "    key_dims=(4, 4, 4),\n",
        "    attn_ratios=(2, 2, 2),\n",
        "    window_sizes=(7, 7, 7),\n",
        "    mlp_ratios=(2, 2, 2),\n",
        "    decoder_dim=32,\n",
        ").to(device)\n",
        "\n",
        "nparams = get_nparams(model)\n",
        "print(f\"Model Parameters: {nparams:,}\")\n",
        "\n",
        "if nparams >= 60000:\n",
        "    print(f\"WARNING: Model has {nparams} parameters, exceeds 60k limit by {nparams - 60000}\")\n",
        "else:\n",
        "    print(f\"Model is within 60k parameter budget: {nparams} < 60000\")\n",
        "\n",
        "# Convert to channels_last for better GPU performance\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Mixed precision training\n",
        "use_amp = torch.cuda.is_available()\n",
        "print(f\"Mixed Precision (AMP): {use_amp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify forward pass\n",
        "print(f\"Verifying forward pass with batch_size={BATCH_SIZE}...\")\n",
        "with torch.no_grad():\n",
        "    test_input = torch.randn(BATCH_SIZE, 1, IMAGE_HEIGHT, IMAGE_WIDTH).to(device, memory_format=torch.channels_last)\n",
        "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "        test_output = model(test_input)\n",
        "    print(f\"Input shape: {test_input.shape}\")\n",
        "    print(f\"Output shape: {test_output.shape}\")\n",
        "    assert test_output.shape == (BATCH_SIZE, 2, IMAGE_HEIGHT, IMAGE_WIDTH), f\"Output shape mismatch!\"\n",
        "    print(\"Forward pass verification: PASSED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer, scheduler, loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5)\n",
        "criterion = CombinedLoss()\n",
        "scaler = torch.amp.GradScaler(\"cuda\") if use_amp else None\n",
        "\n",
        "# Data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = IrisDataset(hf_dataset[\"train\"], split=\"train\", transform=transform)\n",
        "valid_dataset = IrisDataset(hf_dataset[\"validation\"], split=\"validation\", transform=transform)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(valid_dataset)}\")\n",
        "\n",
        "# DataLoaders\n",
        "trainloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "validloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Alpha schedule for loss weighting\n",
        "alpha = np.zeros(EPOCHS)\n",
        "alpha[0:min(125, EPOCHS)] = 1 - np.arange(1, min(125, EPOCHS) + 1) / min(125, EPOCHS)\n",
        "if EPOCHS > 125:\n",
        "    alpha[125:] = 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model: TinyEfficientViT\")\n",
        "print(f\"  Parameters: {nparams:,}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Mixed Precision (AMP): {use_amp}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize metrics tracking\n",
        "train_metrics = {\n",
        "    \"loss\": [], \"iou\": [], \"ce_loss\": [], \"dice_loss\": [], \"surface_loss\": [],\n",
        "    \"alpha\": [], \"lr\": [], \"background_iou\": [], \"pupil_iou\": [],\n",
        "}\n",
        "valid_metrics = {\n",
        "    \"loss\": [], \"iou\": [], \"ce_loss\": [], \"dice_loss\": [], \"surface_loss\": [],\n",
        "    \"background_iou\": [], \"pupil_iou\": [],\n",
        "}\n",
        "best_valid_iou = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Starting training\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss_sum = torch.tensor(0.0, device=device)\n",
        "    train_ce_sum = torch.tensor(0.0, device=device)\n",
        "    train_dice_sum = torch.tensor(0.0, device=device)\n",
        "    train_surface_sum = torch.tensor(0.0, device=device)\n",
        "    train_batch_count = 0\n",
        "    train_intersection = torch.zeros(2, device=device)\n",
        "    train_union = torch.zeros(2, device=device)\n",
        "\n",
        "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for batchdata in pbar:\n",
        "        img, labels, _, spatialWeights, maxDist = batchdata\n",
        "        data = img.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "        target = labels.to(device, non_blocking=True).long()\n",
        "        spatial_weights_gpu = spatialWeights.to(device, non_blocking=True).float()\n",
        "        dist_map_gpu = maxDist.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                output = model(data)\n",
        "                total_loss, ce_loss, dice_loss, surface_loss = criterion(\n",
        "                    output, target, spatial_weights_gpu, dist_map_gpu, alpha[epoch]\n",
        "                )\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            output = model(data)\n",
        "            total_loss, ce_loss, dice_loss, surface_loss = criterion(\n",
        "                output, target, spatial_weights_gpu, dist_map_gpu, alpha[epoch]\n",
        "            )\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss_sum += total_loss.detach()\n",
        "        train_ce_sum += ce_loss.detach()\n",
        "        train_dice_sum += dice_loss.detach()\n",
        "        train_surface_sum += surface_loss.detach()\n",
        "        train_batch_count += 1\n",
        "\n",
        "        predict = get_predictions(output)\n",
        "        batch_intersection, batch_union = compute_iou_tensors(predict, target)\n",
        "        train_intersection += batch_intersection\n",
        "        train_union += batch_union\n",
        "\n",
        "    miou_train, per_class_ious_train = finalize_iou(train_intersection, train_union)\n",
        "    bg_iou_train, pupil_iou_train = per_class_ious_train[0], per_class_ious_train[1]\n",
        "    loss_train = (train_loss_sum / train_batch_count).item()\n",
        "    ce_loss_train = (train_ce_sum / train_batch_count).item()\n",
        "    dice_loss_train = (train_dice_sum / train_batch_count).item()\n",
        "    surface_loss_train = (train_surface_sum / train_batch_count).item()\n",
        "\n",
        "    train_metrics[\"loss\"].append(loss_train)\n",
        "    train_metrics[\"iou\"].append(miou_train)\n",
        "    train_metrics[\"ce_loss\"].append(ce_loss_train)\n",
        "    train_metrics[\"dice_loss\"].append(dice_loss_train)\n",
        "    train_metrics[\"surface_loss\"].append(surface_loss_train)\n",
        "    train_metrics[\"alpha\"].append(alpha[epoch])\n",
        "    train_metrics[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "    train_metrics[\"background_iou\"].append(bg_iou_train)\n",
        "    train_metrics[\"pupil_iou\"].append(pupil_iou_train)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    valid_loss_sum = torch.tensor(0.0, device=device)\n",
        "    valid_ce_sum = torch.tensor(0.0, device=device)\n",
        "    valid_dice_sum = torch.tensor(0.0, device=device)\n",
        "    valid_surface_sum = torch.tensor(0.0, device=device)\n",
        "    valid_batch_count = 0\n",
        "    valid_intersection = torch.zeros(2, device=device)\n",
        "    valid_union = torch.zeros(2, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batchdata in validloader:\n",
        "            img, labels, _, spatialWeights, maxDist = batchdata\n",
        "            data = img.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "            target = labels.to(device, non_blocking=True).long()\n",
        "            spatial_weights_gpu = spatialWeights.to(device, non_blocking=True).float()\n",
        "            dist_map_gpu = maxDist.to(device, non_blocking=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast(\"cuda\"):\n",
        "                    output = model(data)\n",
        "                    total_loss, ce_loss, dice_loss, surface_loss = criterion(\n",
        "                        output, target, spatial_weights_gpu, dist_map_gpu, alpha[epoch]\n",
        "                    )\n",
        "            else:\n",
        "                output = model(data)\n",
        "                total_loss, ce_loss, dice_loss, surface_loss = criterion(\n",
        "                    output, target, spatial_weights_gpu, dist_map_gpu, alpha[epoch]\n",
        "                )\n",
        "\n",
        "            valid_loss_sum += total_loss.detach()\n",
        "            valid_ce_sum += ce_loss.detach()\n",
        "            valid_dice_sum += dice_loss.detach()\n",
        "            valid_surface_sum += surface_loss.detach()\n",
        "            valid_batch_count += 1\n",
        "\n",
        "            predict = get_predictions(output)\n",
        "            batch_intersection, batch_union = compute_iou_tensors(predict, target)\n",
        "            valid_intersection += batch_intersection\n",
        "            valid_union += batch_union\n",
        "\n",
        "    miou_valid, per_class_ious_valid = finalize_iou(valid_intersection, valid_union)\n",
        "    bg_iou_valid, pupil_iou_valid = per_class_ious_valid[0], per_class_ious_valid[1]\n",
        "    loss_valid = (valid_loss_sum / valid_batch_count).item()\n",
        "    ce_loss_valid = (valid_ce_sum / valid_batch_count).item()\n",
        "    dice_loss_valid = (valid_dice_sum / valid_batch_count).item()\n",
        "    surface_loss_valid = (valid_surface_sum / valid_batch_count).item()\n",
        "\n",
        "    valid_metrics[\"loss\"].append(loss_valid)\n",
        "    valid_metrics[\"iou\"].append(miou_valid)\n",
        "    valid_metrics[\"ce_loss\"].append(ce_loss_valid)\n",
        "    valid_metrics[\"dice_loss\"].append(dice_loss_valid)\n",
        "    valid_metrics[\"surface_loss\"].append(surface_loss_valid)\n",
        "    valid_metrics[\"background_iou\"].append(bg_iou_valid)\n",
        "    valid_metrics[\"pupil_iou\"].append(pupil_iou_valid)\n",
        "\n",
        "    scheduler.step(loss_valid)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {loss_train:.4f} | Valid Loss: {loss_valid:.4f}\")\n",
        "    print(f\"Train mIoU: {miou_train:.4f} | Valid mIoU: {miou_valid:.4f}\")\n",
        "    print(f\"Valid BG IoU: {bg_iou_valid:.4f} | Valid Pupil IoU: {pupil_iou_valid:.4f}\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    if miou_valid > best_valid_iou:\n",
        "        best_valid_iou = miou_valid\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model.state_dict(), \"best_efficientvit_model.pt\")\n",
        "        print(f\"New best model! Valid mIoU: {best_valid_iou:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final validation mIoU: {miou_valid:.4f}\")\n",
        "print(f\"Best validation mIoU: {best_valid_iou:.4f} (epoch {best_epoch})\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plot_training_curves(train_metrics, valid_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "model.load_state_dict(torch.load(\"best_efficientvit_model.pt\"))\n",
        "visualize_predictions(model, validloader, device, num_samples=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print final metrics summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL METRICS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model Parameters: {nparams:,}\")\n",
        "print(f\"Parameter Budget: {'PASSED' if nparams < 60000 else 'EXCEEDED'} (<60k)\")\n",
        "print(f\"\")\n",
        "print(f\"Best Validation mIoU: {best_valid_iou:.4f} (epoch {best_epoch})\")\n",
        "print(f\"Final Validation mIoU: {valid_metrics['iou'][-1]:.4f}\")\n",
        "print(f\"Final Train mIoU: {train_metrics['iou'][-1]:.4f}\")\n",
        "print(f\"\")\n",
        "print(f\"Final Validation Loss: {valid_metrics['loss'][-1]:.4f}\")\n",
        "print(f\"Final Train Loss: {train_metrics['loss'][-1]:.4f}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final checkpoint\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': EPOCHS,\n",
        "    'best_valid_iou': best_valid_iou,\n",
        "    'train_metrics': train_metrics,\n",
        "    'valid_metrics': valid_metrics,\n",
        "    'nparams': nparams,\n",
        "}, 'efficientvit_checkpoint.pt')\n",
        "\n",
        "print(\"Checkpoint saved: efficientvit_checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to ONNX\n",
        "model.eval()\n",
        "model_cpu = model.to(\"cpu\").to(memory_format=torch.contiguous_format)\n",
        "\n",
        "dummy_input = torch.randn(1, 1, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model_cpu,\n",
        "    dummy_input,\n",
        "    \"efficientvit_model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=14,\n",
        "    do_constant_folding=True,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        ")\n",
        "\n",
        "print(\"ONNX model exported: efficientvit_model.onnx\")\n",
        "print(f\"ONNX file size: {os.path.getsize('efficientvit_model.onnx') / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download files (run this cell to get download links in Colab)\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading model files...\")\n",
        "files.download('best_efficientvit_model.pt')\n",
        "files.download('efficientvit_checkpoint.pt')\n",
        "files.download('efficientvit_model.onnx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and save models (optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "save_dir = '/content/drive/MyDrive/efficientvit_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "shutil.copy('best_efficientvit_model.pt', save_dir)\n",
        "shutil.copy('efficientvit_checkpoint.pt', save_dir)\n",
        "shutil.copy('efficientvit_model.onnx', save_dir)\n",
        "\n",
        "print(f\"Models saved to Google Drive: {save_dir}\")"
      ]
    }
  ]
}
