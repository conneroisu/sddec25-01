{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ellipse Regression Training - Google Colab\n",
    "\n",
    "This notebook trains an ellipse regression model for pupil detection on the OpenEDS dataset.\n",
    "The model predicts ellipse parameters (center x, center y, radius x, radius y) instead of full semantic segmentation.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU runtime (T4 or better recommended)\n",
    "- ~8GB GPU memory\n",
    "\n",
    "**Dataset:** [Conner/openeds-precomputed](https://huggingface.co/datasets/Conner/openeds-precomputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"Go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q torch torchvision opencv-python-headless datasets huggingface_hub tqdm matplotlib pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "random.seed(12)\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and image configuration\n",
    "HF_DATASET_REPO = \"Conner/openeds-precomputed\"\n",
    "IMAGE_HEIGHT = 400\n",
    "IMAGE_WIDTH = 640\n",
    "\n",
    "# Normalization factors for ellipse parameters\n",
    "MAX_RADIUS = math.sqrt(IMAGE_WIDTH**2 + IMAGE_HEIGHT**2) / 2\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16  # Reduced for Colab free tier (increase to 32 if using Colab Pro)\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 2  # Colab has limited CPU cores\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Performance optimizations\n",
    "USE_AMP = torch.cuda.is_available()  # Automatic mixed precision\n",
    "USE_CHANNELS_LAST = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(12)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache path for Colab persistent storage\n",
    "CACHE_DIR = \"/content/dataset_cache\"\n",
    "CACHE_MARKER = os.path.join(CACHE_DIR, \".cache_complete\")\n",
    "\n",
    "# Check if running in Colab with Google Drive mounted\n",
    "DRIVE_CACHE = \"/content/drive/MyDrive/openeds_cache\"\n",
    "USE_DRIVE_CACHE = os.path.exists(\"/content/drive\")\n",
    "\n",
    "if USE_DRIVE_CACHE:\n",
    "    print(\"Google Drive detected! Using Drive for persistent cache.\")\n",
    "    CACHE_DIR = DRIVE_CACHE\n",
    "    CACHE_MARKER = os.path.join(CACHE_DIR, \".cache_complete\")\n",
    "else:\n",
    "    print(\"No Google Drive mounted. Cache will be lost on runtime disconnect.\")\n",
    "    print(\"To enable persistent cache, run: from google.colab import drive; drive.mount('/content/drive')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (with caching)\n",
    "if os.path.exists(CACHE_MARKER):\n",
    "    print(f\"Loading cached dataset from: {CACHE_DIR}\")\n",
    "    try:\n",
    "        hf_dataset = load_from_disk(CACHE_DIR)\n",
    "        print(\"Loaded from cache!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cache corrupted, re-downloading: {e}\")\n",
    "        import shutil\n",
    "        shutil.rmtree(CACHE_DIR, ignore_errors=True)\n",
    "        hf_dataset = None\n",
    "else:\n",
    "    hf_dataset = None\n",
    "\n",
    "if hf_dataset is None:\n",
    "    print(f\"Downloading from HuggingFace: {HF_DATASET_REPO}\")\n",
    "    print(\"This may take 10-20 minutes on first run...\")\n",
    "    hf_dataset = load_dataset(HF_DATASET_REPO)\n",
    "    \n",
    "    # Save to cache\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    hf_dataset.save_to_disk(CACHE_DIR)\n",
    "    with open(CACHE_MARKER, \"w\") as f:\n",
    "        f.write(f\"Cached from {HF_DATASET_REPO}\\n\")\n",
    "    print(\"Dataset cached!\")\n",
    "\n",
    "print(f\"\\nTrain samples: {len(hf_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(hf_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from dataset\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    sample = hf_dataset['train'][i]\n",
    "    image = np.array(sample['image'], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    label = np.array(sample['label'], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    \n",
    "    axes[0, i].imshow(image, cmap='gray')\n",
    "    axes[0, i].set_title(f\"Image {i}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(label, cmap='jet', vmin=0, vmax=1)\n",
    "    axes[1, i].set_title(f\"Label {i}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Sample images from the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ellipse Parameter Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ellipse_params(mask):\n",
    "    \"\"\"\n",
    "    Extract ellipse parameters from a binary mask.\n",
    "    Returns: (cx, cy, rx, ry, angle) where:\n",
    "        - cx, cy: center coordinates (pixels)\n",
    "        - rx, ry: semi-axes lengths (pixels)\n",
    "        - angle: rotation angle in degrees\n",
    "    If no valid contour found, returns zeros.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(\n",
    "        mask.astype(np.uint8),\n",
    "        cv2.RETR_EXTERNAL,\n",
    "        cv2.CHAIN_APPROX_SIMPLE,\n",
    "    )\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    if len(largest_contour) < 5:\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            cx = M[\"m10\"] / M[\"m00\"]\n",
    "            cy = M[\"m01\"] / M[\"m00\"]\n",
    "            area = cv2.contourArea(largest_contour)\n",
    "            radius = math.sqrt(area / math.pi)\n",
    "            return cx, cy, radius, radius, 0.0\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        ellipse = cv2.fitEllipse(largest_contour)\n",
    "        (cx, cy), (width, height), angle = ellipse\n",
    "        rx = width / 2.0\n",
    "        ry = height / 2.0\n",
    "        return cx, cy, rx, ry, angle\n",
    "    except cv2.error:\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            cx = M[\"m10\"] / M[\"m00\"]\n",
    "            cy = M[\"m01\"] / M[\"m00\"]\n",
    "            area = cv2.contourArea(largest_contour)\n",
    "            radius = math.sqrt(area / math.pi)\n",
    "            return cx, cy, radius, radius, 0.0\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "def normalize_ellipse_params(cx, cy, rx, ry):\n",
    "    \"\"\"Normalize ellipse parameters to [0, 1] range.\"\"\"\n",
    "    cx_norm = cx / IMAGE_WIDTH\n",
    "    cy_norm = cy / IMAGE_HEIGHT\n",
    "    rx_norm = rx / MAX_RADIUS\n",
    "    ry_norm = ry / MAX_RADIUS\n",
    "    return cx_norm, cy_norm, rx_norm, ry_norm\n",
    "\n",
    "\n",
    "def denormalize_ellipse_params(cx_norm, cy_norm, rx_norm, ry_norm):\n",
    "    \"\"\"Denormalize ellipse parameters back to pixel values.\"\"\"\n",
    "    cx = cx_norm * IMAGE_WIDTH\n",
    "    cy = cy_norm * IMAGE_HEIGHT\n",
    "    rx = rx_norm * MAX_RADIUS\n",
    "    ry = ry_norm * MAX_RADIUS\n",
    "    return cx, cy, rx, ry\n",
    "\n",
    "\n",
    "def render_ellipse_mask(cx, cy, rx, ry, height=IMAGE_HEIGHT, width=IMAGE_WIDTH):\n",
    "    \"\"\"Render an ellipse mask from parameters.\"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    if rx > 0 and ry > 0:\n",
    "        cv2.ellipse(\n",
    "            mask,\n",
    "            center=(int(round(cx)), int(round(cy))),\n",
    "            axes=(int(round(rx)), int(round(ry))),\n",
    "            angle=0,\n",
    "            startAngle=0,\n",
    "            endAngle=360,\n",
    "            color=1,\n",
    "            thickness=-1,\n",
    "        )\n",
    "    return mask\n",
    "\n",
    "print(\"Ellipse utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EllipseRegressionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for ellipse regression:\n",
    "    - Smooth L1 for center prediction\n",
    "    - Smooth L1 for radii prediction\n",
    "    - Optional IoU loss computed by rendering ellipses\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, center_weight=1.0, radius_weight=1.0, iou_weight=0.5):\n",
    "        super(EllipseRegressionLoss, self).__init__()\n",
    "        self.center_weight = center_weight\n",
    "        self.radius_weight = radius_weight\n",
    "        self.iou_weight = iou_weight\n",
    "        self.smooth_l1 = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, pred, target, compute_iou=True):\n",
    "        \"\"\"\n",
    "        pred: (B, 4) - cx, cy, rx, ry (normalized)\n",
    "        target: (B, 4) - cx, cy, rx, ry (normalized)\n",
    "        \"\"\"\n",
    "        center_loss = self.smooth_l1(pred[:, :2], target[:, :2])\n",
    "        radius_loss = self.smooth_l1(pred[:, 2:], target[:, 2:])\n",
    "\n",
    "        total_loss = (\n",
    "            self.center_weight * center_loss + self.radius_weight * radius_loss\n",
    "        )\n",
    "\n",
    "        if self.iou_weight > 0 and compute_iou:\n",
    "            param_dist = torch.mean((pred - target) ** 2, dim=1)\n",
    "            iou_proxy_loss = torch.mean(param_dist)\n",
    "            total_loss = total_loss + self.iou_weight * iou_proxy_loss\n",
    "\n",
    "        return total_loss, center_loss, radius_loss\n",
    "\n",
    "\n",
    "def compute_center_error(pred, target):\n",
    "    \"\"\"Compute mean center error in pixels.\"\"\"\n",
    "    pred_cx = pred[:, 0] * IMAGE_WIDTH\n",
    "    pred_cy = pred[:, 1] * IMAGE_HEIGHT\n",
    "    target_cx = target[:, 0] * IMAGE_WIDTH\n",
    "    target_cy = target[:, 1] * IMAGE_HEIGHT\n",
    "    dist = torch.sqrt((pred_cx - target_cx) ** 2 + (pred_cy - target_cy) ** 2)\n",
    "    return dist.mean().item()\n",
    "\n",
    "\n",
    "def compute_radius_error(pred, target):\n",
    "    \"\"\"Compute mean radius error in pixels.\"\"\"\n",
    "    pred_rx = pred[:, 2] * MAX_RADIUS\n",
    "    pred_ry = pred[:, 3] * MAX_RADIUS\n",
    "    target_rx = target[:, 2] * MAX_RADIUS\n",
    "    target_ry = target[:, 3] * MAX_RADIUS\n",
    "    rx_error = torch.abs(pred_rx - target_rx)\n",
    "    ry_error = torch.abs(pred_ry - target_ry)\n",
    "    return ((rx_error + ry_error) / 2).mean().item()\n",
    "\n",
    "\n",
    "def compute_iou_with_gt_mask(pred, gt_masks, device):\n",
    "    \"\"\"\n",
    "    Compute IoU between predicted ellipses and ground truth masks.\n",
    "    gt_masks: (B, H, W) ground truth binary masks\n",
    "    \"\"\"\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    gt_masks_np = gt_masks.cpu().numpy()\n",
    "\n",
    "    batch_size = pred_np.shape[0]\n",
    "    ious_bg = []\n",
    "    ious_pupil = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred_cx, pred_cy, pred_rx, pred_ry = denormalize_ellipse_params(\n",
    "            pred_np[i, 0], pred_np[i, 1], pred_np[i, 2], pred_np[i, 3]\n",
    "        )\n",
    "\n",
    "        pred_mask = render_ellipse_mask(pred_cx, pred_cy, pred_rx, pred_ry)\n",
    "        target_mask = gt_masks_np[i]\n",
    "\n",
    "        # Pupil IoU (class 1)\n",
    "        pred_pupil = pred_mask == 1\n",
    "        target_pupil = target_mask == 1\n",
    "        intersection_pupil = np.logical_and(pred_pupil, target_pupil).sum()\n",
    "        union_pupil = np.logical_or(pred_pupil, target_pupil).sum()\n",
    "        iou_pupil = intersection_pupil / max(union_pupil, 1)\n",
    "        ious_pupil.append(iou_pupil)\n",
    "\n",
    "        # Background IoU (class 0)\n",
    "        pred_bg = pred_mask == 0\n",
    "        target_bg = target_mask == 0\n",
    "        intersection_bg = np.logical_and(pred_bg, target_bg).sum()\n",
    "        union_bg = np.logical_or(pred_bg, target_bg).sum()\n",
    "        iou_bg = intersection_bg / max(union_bg, 1)\n",
    "        ious_bg.append(iou_bg)\n",
    "\n",
    "    mean_bg_iou = np.mean(ious_bg)\n",
    "    mean_pupil_iou = np.mean(ious_pupil)\n",
    "    mean_iou = (mean_bg_iou + mean_pupil_iou) / 2\n",
    "\n",
    "    return mean_iou, mean_bg_iou, mean_pupil_iou\n",
    "\n",
    "\n",
    "print(\"Loss and metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    \"\"\"Encoder block with depthwise separable convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, down_size, dropout=False, prob=0):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.depthwise_conv1 = nn.Conv2d(\n",
    "            input_channels, input_channels, kernel_size=3, padding=1, groups=input_channels\n",
    "        )\n",
    "        self.pointwise_conv1 = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        self.conv21 = nn.Conv2d(\n",
    "            input_channels + output_channels, output_channels, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.depthwise_conv22 = nn.Conv2d(\n",
    "            output_channels, output_channels, kernel_size=3, padding=1, groups=output_channels\n",
    "        )\n",
    "        self.pointwise_conv22 = nn.Conv2d(output_channels, output_channels, kernel_size=1)\n",
    "        self.conv31 = nn.Conv2d(\n",
    "            input_channels + 2 * output_channels, output_channels, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.depthwise_conv32 = nn.Conv2d(\n",
    "            output_channels, output_channels, kernel_size=3, padding=1, groups=output_channels\n",
    "        )\n",
    "        self.pointwise_conv32 = nn.Conv2d(output_channels, output_channels, kernel_size=1)\n",
    "        self.max_pool = nn.AvgPool2d(kernel_size=down_size) if down_size else None\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.down_size = down_size\n",
    "        self.dropout = dropout\n",
    "        self.dropout1 = nn.Dropout(p=prob)\n",
    "        self.dropout2 = nn.Dropout(p=prob)\n",
    "        self.dropout3 = nn.Dropout(p=prob)\n",
    "        self.bn = torch.nn.BatchNorm2d(num_features=output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.max_pool is not None:\n",
    "            x = self.max_pool(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x1 = self.relu(self.dropout1(self.pointwise_conv1(self.depthwise_conv1(x))))\n",
    "            x21 = torch.cat((x, x1), dim=1)\n",
    "            x22 = self.relu(\n",
    "                self.dropout2(self.pointwise_conv22(self.depthwise_conv22(self.conv21(x21))))\n",
    "            )\n",
    "            x31 = torch.cat((x21, x22), dim=1)\n",
    "            out = self.relu(\n",
    "                self.dropout3(self.pointwise_conv32(self.depthwise_conv32(self.conv31(x31))))\n",
    "            )\n",
    "        else:\n",
    "            x1 = self.relu(self.pointwise_conv1(self.depthwise_conv1(x)))\n",
    "            x21 = torch.cat((x, x1), dim=1)\n",
    "            x22 = self.relu(self.pointwise_conv22(self.depthwise_conv22(self.conv21(x21))))\n",
    "            x31 = torch.cat((x21, x22), dim=1)\n",
    "            out = self.relu(self.pointwise_conv32(self.depthwise_conv32(self.conv31(x31))))\n",
    "\n",
    "        return self.bn(out)\n",
    "\n",
    "\n",
    "class EllipseRegressionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight CNN for ellipse parameter regression.\n",
    "    Predicts: (cx, cy, rx, ry) - center and semi-axes of pupil ellipse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1, channel_size=32, dropout=False, prob=0):\n",
    "        super(EllipseRegressionNet, self).__init__()\n",
    "\n",
    "        # Encoder blocks\n",
    "        self.down_block1 = DownBlock(\n",
    "            input_channels=in_channels, output_channels=channel_size,\n",
    "            down_size=None, dropout=dropout, prob=prob,\n",
    "        )\n",
    "        self.down_block2 = DownBlock(\n",
    "            input_channels=channel_size, output_channels=channel_size,\n",
    "            down_size=(2, 2), dropout=dropout, prob=prob,\n",
    "        )\n",
    "        self.down_block3 = DownBlock(\n",
    "            input_channels=channel_size, output_channels=channel_size * 2,\n",
    "            down_size=(2, 2), dropout=dropout, prob=prob,\n",
    "        )\n",
    "        self.down_block4 = DownBlock(\n",
    "            input_channels=channel_size * 2, output_channels=channel_size * 2,\n",
    "            down_size=(2, 2), dropout=dropout, prob=prob,\n",
    "        )\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Regression head\n",
    "        fc_input_size = channel_size * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=prob) if dropout else nn.Identity(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=prob) if dropout else nn.Identity(),\n",
    "            nn.Linear(64, 4),  # cx, cy, rx, ry\n",
    "            nn.Sigmoid(),  # Output in [0, 1] range\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.groups == m.in_channels and m.in_channels == m.out_channels:\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1]\n",
    "                    m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                elif m.kernel_size == (1, 1):\n",
    "                    n = m.in_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                else:\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down_block1(x)\n",
    "        x = self.down_block2(x)\n",
    "        x = self.down_block3(x)\n",
    "        x = self.down_block4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        params = self.fc(x)\n",
    "        return params\n",
    "\n",
    "\n",
    "def get_nparams(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create and inspect model\n",
    "model = EllipseRegressionNet(in_channels=1, channel_size=32, dropout=True, prob=0.2).to(device)\n",
    "nparams = get_nparams(model)\n",
    "print(f\"Model parameters: {nparams:,}\")\n",
    "print(f\"Model size: {nparams * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "if USE_CHANNELS_LAST:\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    print(\"Model converted to channels_last memory format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify forward pass\n",
    "print(f\"Verifying forward pass with batch_size={BATCH_SIZE}...\")\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(BATCH_SIZE, 1, IMAGE_HEIGHT, IMAGE_WIDTH).to(device)\n",
    "    if USE_CHANNELS_LAST:\n",
    "        test_input = test_input.to(memory_format=torch.channels_last)\n",
    "    with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "        test_output = model(test_input)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "    assert test_output.shape == (BATCH_SIZE, 4), f\"Expected ({BATCH_SIZE}, 4), got {test_output.shape}\"\n",
    "    print(\"Forward pass verification: PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip:\n",
    "    def __call__(self, img, label):\n",
    "        if random.random() < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class Gaussian_blur:\n",
    "    def __call__(self, img):\n",
    "        sigma_value = np.random.randint(2, 7)\n",
    "        return Image.fromarray(cv2.GaussianBlur(np.array(img), (7, 7), sigma_value))\n",
    "\n",
    "\n",
    "class Line_augment:\n",
    "    def __call__(self, base):\n",
    "        yc, xc = (0.3 + 0.4 * np.random.rand(1)) * np.array(base.shape)\n",
    "        aug_base = np.copy(base)\n",
    "        num_lines = np.random.randint(1, 10)\n",
    "        for _ in np.arange(0, num_lines):\n",
    "            theta = np.pi * np.random.rand(1)\n",
    "            x1 = xc - 50 * np.random.rand(1) * (1 if np.random.rand(1) < 0.5 else -1)\n",
    "            y1 = (x1 - xc) * np.tan(theta) + yc\n",
    "            x2 = xc - (150 * np.random.rand(1) + 50) * (1 if np.random.rand(1) < 0.5 else -1)\n",
    "            y2 = (x2 - xc) * np.tan(theta) + yc\n",
    "            aug_base = cv2.line(\n",
    "                aug_base, (int(x1), int(y1)), (int(x2), int(y2)), (255, 255, 255), 4\n",
    "            )\n",
    "        aug_base = aug_base.astype(np.uint8)\n",
    "        return Image.fromarray(aug_base)\n",
    "\n",
    "\n",
    "class MaskToTensor:\n",
    "    def __call__(self, img):\n",
    "        return torch.from_numpy(np.array(img, dtype=np.int64)).long()\n",
    "\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, split=\"train\", transform=None):\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n",
    "        self.gamma_table = 255.0 * (np.linspace(0, 1, 256) ** 0.8)\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        image = np.array(sample[\"image\"], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "        label = np.array(sample[\"label\"], dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "        spatial_weights = np.array(sample[\"spatial_weights\"], dtype=np.float32).reshape(\n",
    "            IMAGE_HEIGHT, IMAGE_WIDTH\n",
    "        )\n",
    "        dist_map = np.array(sample[\"dist_map\"], dtype=np.float32).reshape(\n",
    "            2, IMAGE_HEIGHT, IMAGE_WIDTH\n",
    "        )\n",
    "        filename = sample[\"filename\"]\n",
    "\n",
    "        # Extract ellipse parameters from mask\n",
    "        cx, cy, rx, ry, _ = extract_ellipse_params(label)\n",
    "        cx_norm, cy_norm, rx_norm, ry_norm = normalize_ellipse_params(cx, cy, rx, ry)\n",
    "        ellipse_params = torch.tensor([cx_norm, cy_norm, rx_norm, ry_norm], dtype=torch.float32)\n",
    "\n",
    "        # Image preprocessing\n",
    "        pilimg = cv2.LUT(image, self.gamma_table)\n",
    "\n",
    "        if self.transform is not None and self.split == \"train\":\n",
    "            if random.random() < 0.2:\n",
    "                pilimg = Line_augment()(np.array(pilimg))\n",
    "            if random.random() < 0.2:\n",
    "                pilimg = Gaussian_blur()(np.array(pilimg))\n",
    "\n",
    "        img = self.clahe.apply(np.array(np.uint8(pilimg)))\n",
    "        img = Image.fromarray(img)\n",
    "        label_pil = Image.fromarray(label)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            if self.split == \"train\":\n",
    "                img, label_pil = RandomHorizontalFlip()(img, label_pil)\n",
    "                # Check if flipped\n",
    "                if np.array(label_pil)[0, 0] != label[0, 0]:\n",
    "                    spatial_weights = np.fliplr(spatial_weights).copy()\n",
    "                    dist_map = np.flip(dist_map, axis=2).copy()\n",
    "                    # Flip ellipse center x coordinate\n",
    "                    cx_norm = 1.0 - cx_norm\n",
    "                    ellipse_params = torch.tensor(\n",
    "                        [cx_norm, cy_norm, rx_norm, ry_norm], dtype=torch.float32\n",
    "                    )\n",
    "\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label_tensor = MaskToTensor()(label_pil)\n",
    "\n",
    "        return (img, label_tensor, ellipse_params, filename, spatial_weights, dist_map)\n",
    "\n",
    "\n",
    "print(\"Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = IrisDataset(hf_dataset[\"train\"], split=\"train\", transform=transform)\n",
    "valid_dataset = IrisDataset(hf_dataset[\"validation\"], split=\"validation\", transform=transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "validloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining batches: {len(trainloader)}\")\n",
    "print(f\"Validation batches: {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer, scheduler, and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5)\n",
    "criterion = EllipseRegressionLoss(center_weight=1.0, radius_weight=1.0, iou_weight=0.5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n",
    "\n",
    "# Metrics storage\n",
    "train_metrics = {\n",
    "    \"loss\": [], \"iou\": [], \"center_loss\": [], \"radius_loss\": [],\n",
    "    \"center_error\": [], \"radius_error\": [], \"lr\": [],\n",
    "}\n",
    "valid_metrics = {\n",
    "    \"loss\": [], \"iou\": [], \"center_loss\": [], \"radius_loss\": [],\n",
    "    \"center_error\": [], \"radius_error\": [],\n",
    "}\n",
    "\n",
    "best_valid_iou = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Mixed Precision (AMP): {USE_AMP}\")\n",
    "print(f\"  Channels Last: {USE_CHANNELS_LAST}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_center_loss_sum = 0.0\n",
    "    train_radius_loss_sum = 0.0\n",
    "    train_batch_count = 0\n",
    "    train_center_errors = []\n",
    "    train_radius_errors = []\n",
    "    train_ious = []\n",
    "\n",
    "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batchdata in pbar:\n",
    "        img, labels, ellipse_params, _, _, _ = batchdata\n",
    "\n",
    "        data = img.to(device, non_blocking=True)\n",
    "        if USE_CHANNELS_LAST:\n",
    "            data = data.to(memory_format=torch.channels_last)\n",
    "        target_params = ellipse_params.to(device, non_blocking=True)\n",
    "        target_labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                output = model(data)\n",
    "                total_loss, center_loss, radius_loss = criterion(output, target_params)\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            output = model(data)\n",
    "            total_loss, center_loss, radius_loss = criterion(output, target_params)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_sum += total_loss.item()\n",
    "        train_center_loss_sum += center_loss.item()\n",
    "        train_radius_loss_sum += radius_loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "        train_center_errors.append(compute_center_error(output, target_params))\n",
    "        train_radius_errors.append(compute_radius_error(output, target_params))\n",
    "        miou, _, _ = compute_iou_with_gt_mask(output, target_labels, device)\n",
    "        train_ious.append(miou)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\", \"iou\": f\"{miou:.4f}\"})\n",
    "\n",
    "    # Training metrics\n",
    "    loss_train = train_loss_sum / train_batch_count\n",
    "    miou_train = np.mean(train_ious)\n",
    "    train_metrics[\"loss\"].append(loss_train)\n",
    "    train_metrics[\"iou\"].append(miou_train)\n",
    "    train_metrics[\"center_loss\"].append(train_center_loss_sum / train_batch_count)\n",
    "    train_metrics[\"radius_loss\"].append(train_radius_loss_sum / train_batch_count)\n",
    "    train_metrics[\"center_error\"].append(np.mean(train_center_errors))\n",
    "    train_metrics[\"radius_error\"].append(np.mean(train_radius_errors))\n",
    "    train_metrics[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_loss_sum = 0.0\n",
    "    valid_center_loss_sum = 0.0\n",
    "    valid_radius_loss_sum = 0.0\n",
    "    valid_batch_count = 0\n",
    "    valid_center_errors = []\n",
    "    valid_radius_errors = []\n",
    "    valid_ious = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batchdata in validloader:\n",
    "            img, labels, ellipse_params, _, _, _ = batchdata\n",
    "\n",
    "            data = img.to(device, non_blocking=True)\n",
    "            if USE_CHANNELS_LAST:\n",
    "                data = data.to(memory_format=torch.channels_last)\n",
    "            target_params = ellipse_params.to(device, non_blocking=True)\n",
    "            target_labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    output = model(data)\n",
    "                    total_loss, center_loss, radius_loss = criterion(\n",
    "                        output, target_params, compute_iou=False\n",
    "                    )\n",
    "            else:\n",
    "                output = model(data)\n",
    "                total_loss, center_loss, radius_loss = criterion(\n",
    "                    output, target_params, compute_iou=False\n",
    "                )\n",
    "\n",
    "            valid_loss_sum += total_loss.item()\n",
    "            valid_center_loss_sum += center_loss.item()\n",
    "            valid_radius_loss_sum += radius_loss.item()\n",
    "            valid_batch_count += 1\n",
    "\n",
    "            valid_center_errors.append(compute_center_error(output, target_params))\n",
    "            valid_radius_errors.append(compute_radius_error(output, target_params))\n",
    "            miou, _, _ = compute_iou_with_gt_mask(output, target_labels, device)\n",
    "            valid_ious.append(miou)\n",
    "\n",
    "    # Validation metrics\n",
    "    loss_valid = valid_loss_sum / valid_batch_count\n",
    "    miou_valid = np.mean(valid_ious)\n",
    "    valid_metrics[\"loss\"].append(loss_valid)\n",
    "    valid_metrics[\"iou\"].append(miou_valid)\n",
    "    valid_metrics[\"center_loss\"].append(valid_center_loss_sum / valid_batch_count)\n",
    "    valid_metrics[\"radius_loss\"].append(valid_radius_loss_sum / valid_batch_count)\n",
    "    valid_metrics[\"center_error\"].append(np.mean(valid_center_errors))\n",
    "    valid_metrics[\"radius_error\"].append(np.mean(valid_radius_errors))\n",
    "\n",
    "    scheduler.step(loss_valid)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {loss_train:.4f} | Valid Loss: {loss_valid:.4f}\")\n",
    "    print(f\"  Train mIoU: {miou_train:.4f} | Valid mIoU: {miou_valid:.4f}\")\n",
    "    print(f\"  Center Error: {np.mean(valid_center_errors):.2f}px | Radius Error: {np.mean(valid_radius_errors):.2f}px\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if miou_valid > best_valid_iou:\n",
    "        best_valid_iou = miou_valid\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), \"best_ellipse_model.pt\")\n",
    "        print(f\"  New best model saved! Valid mIoU: {best_valid_iou:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Valid mIoU: {best_valid_iou:.4f} (Epoch {best_epoch})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs_range = range(1, len(train_metrics[\"loss\"]) + 1)\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(epochs_range, train_metrics[\"loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, valid_metrics[\"loss\"], \"r-\", label=\"Valid Loss\", linewidth=2)\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training and Validation Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU curves\n",
    "axes[0, 1].plot(epochs_range, train_metrics[\"iou\"], \"b-\", label=\"Train mIoU\", linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, valid_metrics[\"iou\"], \"r-\", label=\"Valid mIoU\", linewidth=2)\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"mIoU\")\n",
    "axes[0, 1].set_title(\"Training and Validation mIoU\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Center error\n",
    "axes[1, 0].plot(epochs_range, train_metrics[\"center_error\"], \"b-\", label=\"Train\", linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, valid_metrics[\"center_error\"], \"r-\", label=\"Valid\", linewidth=2)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Error (pixels)\")\n",
    "axes[1, 0].set_title(\"Center Error\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Radius error\n",
    "axes[1, 1].plot(epochs_range, train_metrics[\"radius_error\"], \"b-\", label=\"Train\", linewidth=2)\n",
    "axes[1, 1].plot(epochs_range, valid_metrics[\"radius_error\"], \"r-\", label=\"Valid\", linewidth=2)\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Error (pixels)\")\n",
    "axes[1, 1].set_title(\"Radius Error\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "num_samples = 4\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(validloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        \n",
    "        img, labels, ellipse_params, _, _, _ = batch\n",
    "        single_img = img[0:1].to(device)\n",
    "        if USE_CHANNELS_LAST:\n",
    "            single_img = single_img.to(memory_format=torch.channels_last)\n",
    "        \n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            output = model(single_img)\n",
    "        \n",
    "        pred_params = output[0].cpu().numpy()\n",
    "        pred_cx, pred_cy, pred_rx, pred_ry = denormalize_ellipse_params(\n",
    "            pred_params[0], pred_params[1], pred_params[2], pred_params[3]\n",
    "        )\n",
    "        pred_mask = render_ellipse_mask(pred_cx, pred_cy, pred_rx, pred_ry)\n",
    "        \n",
    "        axes[i, 0].imshow(img[0].squeeze().numpy(), cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"Input Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(labels[0].numpy(), cmap=\"jet\", vmin=0, vmax=1)\n",
    "        axes[i, 1].set_title(\"Ground Truth\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        axes[i, 2].imshow(pred_mask, cmap=\"jet\", vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title(f\"Prediction\\ncx={pred_cx:.1f}, cy={pred_cy:.1f}\\nrx={pred_rx:.1f}, ry={pred_ry:.1f}\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"predictions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predictions saved to predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metrics summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Model Performance (Epoch {best_epoch}):\")\n",
    "print(f\"  Validation mIoU: {best_valid_iou:.4f}\")\n",
    "print(f\"\\nFinal Epoch Metrics:\")\n",
    "print(f\"  Train Loss: {train_metrics['loss'][-1]:.4f}\")\n",
    "print(f\"  Valid Loss: {valid_metrics['loss'][-1]:.4f}\")\n",
    "print(f\"  Train mIoU: {train_metrics['iou'][-1]:.4f}\")\n",
    "print(f\"  Valid mIoU: {valid_metrics['iou'][-1]:.4f}\")\n",
    "print(f\"  Center Error: {valid_metrics['center_error'][-1]:.2f} pixels\")\n",
    "print(f\"  Radius Error: {valid_metrics['radius_error'][-1]:.2f} pixels\")\n",
    "print(f\"\\nModel saved as: best_ellipse_model.pt\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and export to ONNX\n",
    "model.load_state_dict(torch.load(\"best_ellipse_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Convert to contiguous format for export\n",
    "model_export = model.to(memory_format=torch.contiguous_format)\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 1, IMAGE_HEIGHT, IMAGE_WIDTH).to(device)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"ellipse_regression_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model_export,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"ONNX model exported to: {onnx_path}\")\n",
    "print(f\"File size: {os.path.getsize(onnx_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ONNX model\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Load and check ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model validation: PASSED\")\n",
    "    \n",
    "    # Test inference with ONNX Runtime\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    test_input = np.random.randn(1, 1, IMAGE_HEIGHT, IMAGE_WIDTH).astype(np.float32)\n",
    "    ort_output = ort_session.run(None, {\"input\": test_input})\n",
    "    print(f\"ONNX Runtime inference test: PASSED\")\n",
    "    print(f\"Output shape: {ort_output[0].shape}\")\n",
    "except ImportError:\n",
    "    print(\"Install onnx and onnxruntime to verify: pip install onnx onnxruntime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading trained models...\")\n",
    "    files.download(\"best_ellipse_model.pt\")\n",
    "    files.download(\"ellipse_regression_model.onnx\")\n",
    "    files.download(\"training_curves.png\")\n",
    "    files.download(\"predictions.png\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab.\")\n",
    "    print(\"\\nModel files saved locally:\")\n",
    "    print(\"  - best_ellipse_model.pt\")\n",
    "    print(\"  - ellipse_regression_model.onnx\")\n",
    "    print(\"  - training_curves.png\")\n",
    "    print(\"  - predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Notes\n",
    "\n",
    "### Running on Google Colab\n",
    "1. Go to [Google Colab](https://colab.research.google.com/)\n",
    "2. Upload this notebook or open from GitHub\n",
    "3. Set runtime to GPU: `Runtime -> Change runtime type -> T4 GPU`\n",
    "4. Run all cells in order\n",
    "\n",
    "### Persistent Storage (Optional)\n",
    "To save the dataset cache across sessions:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "### Adjusting Hyperparameters\n",
    "- **Colab Free (T4)**: `BATCH_SIZE=16`, `NUM_WORKERS=2`\n",
    "- **Colab Pro (A100)**: `BATCH_SIZE=32-64`, `NUM_WORKERS=4`\n",
    "\n",
    "### Model Output\n",
    "The model predicts 4 normalized values: `[cx, cy, rx, ry]`\n",
    "- `cx, cy`: Ellipse center (normalized by image dimensions)\n",
    "- `rx, ry`: Semi-axes radii (normalized by max radius)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
