\chapter{Testing}\label{chap:testing}

\section{Testing Strategy Overview}\label{sec:testing-overview}

Comprehensive validation ensured the system met critical performance
objectives: fast processing (<16.6ms between frames) and high accuracy (99.8\%).

\begin{quote}
  \textit{Note: Numerical values are representative placeholders due
  to NDA restrictions.}
\end{quote}

\subsection{Testing Philosophy}\label{subsec:testing-philosophy}

Early and continuous testing enabled rapid issue detection and resolution.
This methodology involved:

\begin{itemize}
  \item Testing each pipeline stage and the U-Net algorithm
    components as they were created
  \item Checking memory use and buffer management before building the
    full system
  \item Testing DPU access scheduling during development
\end{itemize}

\subsection{Testing Challenges}\label{subsec:testing-challenges}

Testing challenges encountered:

\begin{itemize}
  \item Testing on FPGA hardware was different from normal software testing
  \item Ensuring pipe-lined threads and DPU scheduling worked together correctly
  \item Balancing speed and accuracy
  \item Verifying efficient memory use for pipe-lined data flow
\end{itemize}

\subsection{Testing Schedule}\label{subsec:testing-schedule}

\begin{itemize}
  \item \textbf{Weeks 1--2}: Test individual parts
  \item \textbf{Weeks 3--4}: Test how parts connect
  \item \textbf{Weeks 5--6}: Test complete system
  \item \textbf{Weeks 7--8}: Test under different conditions
  \item \textbf{Weeks 9--10}: Final testing
\end{itemize}

\section{Unit Testing}\label{sec:unit-testing}

\subsection{Feature Map Testing}\label{subsec:feature-map-testing}

\begin{itemize}
  \item Comprehensive validation that feature maps match between
    unified and scheduled implementations
  \item Layer-by-layer comparison to ensure mathematical consistency
    throughout the network
  \item Statistical analysis of feature map similarity using 80--20
    training/testing dataset split
  \item Verification of feature activation patterns across diverse
    input conditions
\end{itemize}

\subsection{Algorithm Testing}\label{subsec:algorithm-testing}

\begin{itemize}
  \item Resource allocation verification to confirm fair DPU access distribution
  \item Temporal analysis of periodic data collection to ensure
    deadlines are consistently met
  \item Controlled stress testing to verify scheduling robustness
    under varying load conditions
  \item Validation that algorithms 1, 2, and 3 can reliably collect
    data without interruption
\end{itemize}

\subsection{System Coordination
Testing}\label{subsec:system-coordination-testing}

\begin{itemize}
  \item \textbf{Operational Timing}: Verify system meets timing
    requirements for all components
  \item \textbf{Resource Utilization}: Validate efficient use of
    system resources
  \item \textbf{System Stability}: Ensure reliable operation under
    various conditions
\end{itemize}

\subsection{Success Goals}\label{subsec:success-goals}

\begin{itemize}
  \item 100\% feature map consistency between unified algorithm and
    scheduled implementation
  \item Zero missed periodic data collection deadlines across
    extended operation periods
  \item Resource utilization efficiency improvement of at least 30\%
    compared to sequential approach
\end{itemize}

\section{Interface Testing}\label{sec:interface-testing}

\subsection{Key Interfaces}\label{subsec:key-interfaces}

\begin{enumerate}
  \item \textbf{Between Algorithms and Scheduler:}
    \begin{itemize}
      \item Verification of request handling under varying load
        conditions and priorities
      \item Validation of preemption mechanisms when periodic
        collection deadlines approached
      \item Confirmation that all algorithms received their
        guaranteed resource allocation minimums
      \item Analysis of scheduling fairness across extended operational periods
    \end{itemize}

  \item \textbf{Between Semantic Segmentation and DPU:}
    \begin{itemize}
      \item Detailed profiling of resource utilization patterns
        during algorithm execution
      \item Verification that feature map integrity was maintained
        despite scheduled access
      \item Measurement of context switching overhead to ensure
        minimal performance impact
      \item Confirmation that unified algorithm behavior remained consistent
    \end{itemize}

  \item \textbf{Memory Management:}
    \begin{itemize}
      \item Testing how each algorithm accessed its assigned memory
      \item Verifying that memory access patterns were efficient and
        minimized contention
      \item Validating that shared memory regions were properly protected
    \end{itemize}

  \item \textbf{Thread Coordination:}
    \begin{itemize}
      \item Testing how the scheduler managed resource allocation
      \item Verifying that priority escalation worked properly for
        deadline-sensitive operations
      \item Validating synchronization between algorithms with interdependencies
    \end{itemize}
\end{enumerate}

\subsection{Test Cases}\label{subsec:test-cases}

\begin{enumerate}
  \item \textbf{Data Processing Validation:}
    \begin{itemize}
      \item \textbf{Purpose}: Ensured accurate image processing under
        various operating conditions
      \item \textbf{Expected outcome}: Consistent processing quality
        matching baseline performance
      \item \textbf{Validation method}: Comparison against
        established accuracy metrics
    \end{itemize}

  \item \textbf{Resource Management Validation:}
    \begin{itemize}
      \item \textbf{Purpose}: Verified system stability under
        concurrent processing demands
      \item \textbf{Expected outcome}: Reliable operation without
        resource conflicts
      \item \textbf{Validation method}: Performance monitoring during
        multi-algorithm execution
    \end{itemize}

  \item \textbf{Periodic Collection Test:}
    \begin{itemize}
      \item \textbf{Procedure}: System was executed under load with
        varying periodic collection requirements
      \item \textbf{Expected Outcome}: All algorithms met their
        collection deadlines
      \item \textbf{Validation Method}: Collection times were logged
        and verified against specified requirements
    \end{itemize}
\end{enumerate}

\section{System Testing}\label{sec:system-testing}

\subsection{Test Plan}\label{subsec:test-plan}

The following test plan validated the demonstration system (separate from
the client solution). All critical performance, accuracy, and reliability
requirements were tested with scenario coverage representative of real-world
conditions.

\begin{enumerate}
  \item \textbf{Continuous Running Test:}
    \begin{itemize}
      \item \textbf{Procedure}: Multiple eye images were processed
        continuously via an image generator with logging
      \item \textbf{Objective}: Maintained 16.6 ms frame interval
        throughout execution periods exceeding 30 minutes
    \end{itemize}

  \item \textbf{Lighting Test:}
    \begin{itemize}
      \item \textbf{Procedure}: Images with varying lighting
        conditions were evaluated using a comprehensive dataset
      \item \textbf{Objective}: Maintained accuracy above 98\% across
        all lighting conditions
    \end{itemize}

  \item \textbf{Stress Test:}
    \begin{itemize}
      \item \textbf{Procedure}: Memory and processing limits were
        tested using automated stress testing scripts
      \item \textbf{Objective}: System maintained operational
        stability without failure
    \end{itemize}

  \item \textbf{Long-Term Test:}
    \begin{itemize}
      \item \textbf{Procedure}: Automated testing with monitoring was
        conducted over extended periods of 24+ hours
      \item \textbf{Objective}: System operated without crashes or
        performance degradation over extended runtime
    \end{itemize}
\end{enumerate}

\subsection{Test Measurements}\label{subsec:test-measurements}

\begin{itemize}
  \item \textbf{Speed}: Frames per second (goal: >60)
  \item \textbf{Accuracy}: Correct pupil tracking (goal: >98\%)
  \item \textbf{Time}: Input to output delay (goal: 60 frames per second)
  \item \textbf{Memory}: How much memory is used over time
  \item \textbf{Stability}: How long the system runs without problems
\end{itemize}

\section{Regression Testing}\label{sec:regression-testing}

\subsection{Automated Testing}\label{subsec:automated-testing}

Automated CI regression testing was not implemented due to hardware access
constraints. Individual tests used automated scripts but required manual
initiation. Limitations preventing CI workflow:

  \begin{itemize}
    \item \textbf{Hardware Location}: The AMD Kria KV260 was deployed at
      the client location, not team-accessible
    \item \textbf{Network Requirements}: Remote access required ISU VPN,
      incompatible with standard CI/CD infrastructure
    \item \textbf{Physical Access Constraints}: Testing required direct
      hardware access for deployment and debugging
  \end{itemize}

With co-located hardware, automated regression tests would include:

\begin{enumerate}
  \item \textbf{Performance Check}: Compare processing speed to
    previous test runs
    \begin{itemize}
      \item \textbf{Tool}: Test runner with historical performance database
    \end{itemize}

  \item \textbf{Accuracy Check}: Verify algorithm changes maintain
    segmentation accuracy
    \begin{itemize}
      \item \textbf{Tool}: Validation against test dataset with known
        ground truth
    \end{itemize}

  \item \textbf{Resource Check}: Ensure changes do not increase memory
    or CPU utilization beyond acceptable thresholds
    \begin{itemize}
      \item \textbf{Tool}: Vitis-AI Profiler with automated logging
        and comparison
    \end{itemize}
\end{enumerate}

Regression testing was conducted manually via VPN connection, providing
equivalent validation but increased testing latency.

\subsection{Monitoring}\label{subsec:monitoring}

\textbf{Performance Tracking}: Use Vitis-AI
Profiler~\cite{amd2023vitis} to watch:
\begin{itemize}
  \item Running time
  \item DPU use
  \item Memory use
  \item Thread timing
\end{itemize}

\section{Integration Testing}\label{sec:integration-testing}

\subsection{Multi-Algorithm
Integration}\label{subsec:multi-algorithm-integration}

\begin{itemize}
  \item Verified all algorithms (1, 2, 3, and semantic segmentation)
    worked together without conflicts
  \item Tested priority escalation and resource reallocation under
    deadline pressure
  \item Validated end-to-end data flow from image capture to assistive response
\end{itemize}

\subsection{Hardware-Software
Integration}\label{subsec:hardware-software-integration}

\begin{itemize}
  \item Tested camera integration and image capture reliability
  \item Verified DPU scheduling worked correctly with the FPGA bitstream configuration
  \item Validated memory management across hardware and software boundaries
\end{itemize}

\section{Performance Testing}\label{sec:performance-testing}

\subsection{Benchmarking}\label{subsec:benchmarking}

\begin{itemize}
  \item Compared optimized performance against baseline implementation
  \item Measured throughput improvements across different input conditions
  \item Validated resource utilization efficiency improvements
\end{itemize}


\section{Quality Assurance}\label{sec:quality-assurance}

\subsection{IEEE Standards Compliance}\label{subsec:ieee-compliance}

The testing methodology was designed in accordance with applicable IEEE standards for AI-based medical devices:

\begin{itemize}
  \item \textbf{IEEE 3129--2023}~\cite{ieee3129-2023}: Our robustness
    testing follows guidelines for AI-based image recognition systems,
    including lighting variation, stress scenarios, and long-term stability.

  \item \textbf{IEEE 2802--2022}~\cite{ieee2802-2022}: Our evaluation
    methodology follows terminology and best practices for AI-based
    medical devices, focusing on throughput, accuracy, and latency.
\end{itemize}

\subsection{Test Documentation}\label{subsec:test-documentation}

\begin{itemize}
  \item Comprehensive test case documentation with expected results
  \item Performance baseline establishment and tracking
  \item Issue tracking and resolution documentation
  \item Regression test maintenance and evolution
\end{itemize}
