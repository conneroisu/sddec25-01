\chapter{Design}\label{chap:design}

\section{Design Context}\label{sec:design-context}

\subsection{Broader Context}\label{subsec:broader-context}

Our project addresses the needs of individuals with mobility impairments
who require eye-tracking systems for assistive device control. We design
for healthcare professionals, caregivers, and individuals with conditions
such as cerebral palsy who depend on responsive eye tracking.

\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \caption{Design Context Analysis}
  \begin{tabular}{p{2.5cm}p{4.5cm}p{5.5cm}}
    \toprule
    \textbf{Area}
    & \textbf{Description} & \textbf{Examples} \\
    \midrule
    Public health, safety, and welfare
    &
    Our project directly improves the safety and well-being of
    individuals with mobility impairments by enhancing the
    responsiveness of eye-tracking medical monitoring systems. &
    Faster response times to potential medical issues, more reliable
    detection of eye movements for wheelchair control, reduced risk
    of incidents for users
    \\
    \midrule
    Global, cultural, and social
    &
    The solution respects the values of independence and dignity for
    people with disabilities while acknowledging the cultural
    practices around care and assistance.             &
    Supports the right to autonomy for people with disabilities,
    aligns with medical ethics of
    beneficence~\cite{beauchamp2007ethics}, works within existing
    healthcare frameworks                                          \\
    \midrule
    Ecological
    &
    By optimizing software rather than requiring new hardware, our
    solution extends the useful life of existing devices and reduces
    electronic waste.                            &
    Reduced need for frequent hardware replacement, lower energy
    consumption through optimized processing
    \\
    \midrule
    Economic
    &
    Our optimization approach provides significant performance
    improvements while keeping costs accessible for healthcare
    providers and individuals.                             &
    Affordable enhancement to existing assistive technology systems,
    more efficient use of available computing resources, potential
    reduction in healthcare costs through preventative monitoring
    \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Prior Work and Solutions}\label{subsec:prior-work}

Several approaches implement semantic segmentation for eye
tracking~\cite{garcia2017review}, but most face limitations on
resource-constrained edge devices:

\begin{enumerate}
  \item \textbf{Wang et al. (2021)}~\cite{wang2021} proposed
    ``EfficientEye: A Lightweight Semantic Segmentation Framework for
    Eye Tracking'', which achieved good accuracy but still required
    substantial computational resources. Their approach reduced model
    size but processing speed remained at approximately 120 ms per frame.

  \item \textbf{Previous Project Iteration} implemented a standard
    U-Net architecture on the Kria KV260 board with high accuracy
    (99.8\% IoU) but could only process a single frame every 160ms,
    which is insufficient for real-time application needs.

  \item \textbf{Commercial Solutions} like Tobii Pro Fusion offer
    high-speed eye tracking (250 Hz) but require dedicated hardware
    and specialized processors, making them expensive and difficult
    to integrate into existing assistive devices.
\end{enumerate}

\subsubsection{Advantages of Our Approach}
\begin{itemize}
  \item Maintains high accuracy while significantly improving processing speed
  \item Utilizes existing hardware (Kria KV260) without requiring
    costly upgrades
  \item Implements a pipelined approach that maximizes system
    throughput through overlapped CPU and DPU processing
  \item Integrates with existing assistive wheelchair technology ecosystem
\end{itemize}

\subsubsection{Limitations of Our Approach}
\begin{itemize}
  \item Requires careful optimization of memory and DPU resources
  \item Complexity in thread synchronization and pipeline management
  \item Dependent on specific hardware architecture (Kria KV260)
\end{itemize}

\subsection{Technical Complexity}\label{subsec:technical-complexity}

Our project demonstrates technical complexity in components and requirements:

\begin{enumerate}
  \item \textbf{Multiple Components with Distinct Scientific Principles:}
    \begin{itemize}
      \item \textbf{Neural Network Architecture:} The U-Net semantic
        segmentation algorithm incorporates complex convolutional
        neural network principles with encoder-decoder
        architecture~\cite{burden2013}
      \item \textbf{Pipelined Computing:} Implementation of
        multithreaded CPU processing and pipeline architecture
        leverages computer architecture principles~\cite{zhao2023parallel}
      \item \textbf{Memory Management:} Developing efficient memory
        allocation strategies based on computer systems principles
      \item \textbf{Real-time Systems:} Balancing processing load to
        meet strict timing constraints based on real-time systems theory
      \item \textbf{Resource Scheduling:} Creating optimal sequential
        DPU scheduling mechanisms based on operating systems
        principles, managing the constraint that the DPU can only
        execute one xmodel inference at a time
    \end{itemize}

  \item \textbf{Challenging Requirements:}
    \begin{itemize}
      \item \textbf{Speed Improvement:} Achieving near 5x throughput
        improvement through pipelining exceeds typical optimization
        gains in the industry
      \item \textbf{Accuracy Maintenance:} Preserving high accuracy
        while implementing complex pipelined processing is
        significantly more challenging than standard optimization
      \item \textbf{Resource Constraints:} Working within the limited
        memory (4GB) and single-DPU architecture of the Kria board
        requires innovative scheduling solutions
      \item \textbf{Real-time Performance:} Meeting the 60 frames per
        second requirement is at the upper end of what is possible
        with current embedded AI systems
    \end{itemize}
\end{enumerate}

Maintaining accuracy while implementing efficient pipelined execution
with resource scheduling represents complexity beyond standard solutions.

\section{Design Exploration}\label{sec:design-exploration}

\subsection{Design Decisions}\label{subsec:design-decisions}

Key design decisions critical to project success:

\begin{enumerate}
  \item \textbf{Resource Scheduling Approach}
    \begin{itemize}
      \item \textbf{Decision:} Implement an efficient sequential
        scheduling system for DPU access, recognizing that the DPU
        can only execute one xmodel inference at a time. The system
        uses round-robin or priority-based scheduling to coordinate
        access among semantic segmentation and other algorithms.
      \item \textbf{Importance:} Fundamental to throughput goals while
        ensuring blink detection and eye tracking collect required data.
        Without effective scheduling, semantic segmentation would monopolize
        the DPU\@. The approach must provide fair allocation while maintaining
        99.8\% IoU accuracy.
    \end{itemize}

  \item \textbf{DPU Access Management}
    \begin{itemize}
      \item \textbf{Decision:} Implement a fair access scheduling
        approach that prevents semantic segmentation from `starving'
        other algorithms of DPU resources.
      \item \textbf{Importance:} The Kria board has four DDR4 memory
        banks (1GB each), but the single DPU must be carefully managed.
        Our approach ensures semantic segmentation doesn't prevent other
        algorithms from collecting periodic data, avoiding incorrect
        information from delayed collection.
    \end{itemize}

  \item \textbf{Resource Allocation Strategy}
    \begin{itemize}
      \item \textbf{Decision:} Develop a resource management system
        that coordinates access to the DPU and ensures each algorithm
        receives appropriate processing time.
      \item \textbf{Importance:} With multiple algorithms needing DPU
        access, proper allocation prevents starvation and maintains data
        integrity. Blink detection and eye tracking require periodic data
        or information becomes incorrect. Scheduling must prevent semantic
        segmentation from monopolizing resources.
    \end{itemize}

  \item \textbf{Single DPU Architecture Constraint}
    \begin{itemize}
      \item \textbf{Decision:} Utilize the single B4096 DPU core
        (300MHz) on the KV260 board rather than exploring dual
        smaller DPU configurations.
      \item \textbf{Rationale:} The KV260's FPGA fabric supports two
        smaller DPUs (B1024 or B512) instead of the single B4096 core.
        Both our team and a previous ISU team concluded dual DPUs would
        be advantageous. We presented this with model size justifications,
        but the client directed maintaining the single B4096 architecture.
      \item \textbf{Importance:} This constraint prevents hardware-level
        parallelism. We must focus on time-division scheduling and resource
        sharing, requiring careful coordination of algorithm access patterns
        to prevent contention while meeting throughput targets.
    \end{itemize}
\end{enumerate}

\subsection{Ideation}\label{subsec:ideation}

We explored several scheduling strategies:

\begin{enumerate}
  \item \textbf{Round-Robin Scheduling}
    \begin{itemize}
      \item Allocate DPU time in equal slices to each algorithm in
        circular order
      \item Simple to implement and ensures each algorithm gets fair access
      \item May not be optimal for variable processing requirements
    \end{itemize}

  \item \textbf{Priority-Based Scheduling}
    \begin{itemize}
      \item Assign priority levels to algorithms based on urgency of
        data collection needs
      \item Higher priority tasks preempt lower priority ones when necessary
      \item Could be tuned to ensure periodic data collection
        requirements are met
    \end{itemize}

  \item \textbf{Time-Division Multiplexing}
    \begin{itemize}
      \item Allocate specific time windows for each algorithm to access the DPU
      \item Synchronize windows with periodic data collection requirements
      \item Optimizes for predictable execution patterns
    \end{itemize}
\end{enumerate}

Options generated through brainstorming, literature review, and analysis
of periodic data requirements.

\subsection{Decision-Making and Trade-Off}\label{subsec:decision-making}

The client required maintaining accuracy with no degradation due to
medical sensitivity. Blink detection and eye tracking require periodic
data collection; semantic segmentation cannot starve other algorithms.

Our design prioritizes balanced performance while ensuring all components
meet operational requirements, maintaining critical timing for essential
functions while achieving target throughput.

The resource management strategy ensures all components receive appropriate
resources based on importance and timing requirements.

For embedded deployment, we selected a scheduling approach minimizing
memory transfer overhead while ensuring periodic data collection. This
is feasible because our model uses fixed memory access patterns with
predictable resource requirements.

The scheduling system must also account for OS tasks and other ML algorithms,
allocating appropriate DPU time slices for these workloads.

\section{Proposed Design}\label{sec:proposed-design}

\subsection{Overview}\label{subsec:overview}

Our project enhances the U-Net-based eye tracking system for individuals
with disabilities. The system monitors eye movements to detect potential
medical issues and automatically repositions users to prevent incidents.

The current implementation processes a single frame in 160ms, insufficient
for real-time monitoring. Our pipelined architecture with multithreaded CPU
and sequential DPU execution achieves approximately 8.3ms per frame average
(33.2ms total for 4 frames), a nearly 5x speed increase.

At a high level, our system:

\begin{enumerate}
  \item Captures eye movement images through a camera
  \item Processes these images using pipelined semantic segmentation
    to remove reflections and identify the pupil
  \item Tracks the eye's position and detects blinks in real-time
  \item Provides this information to the assistive wheelchair
    technology for appropriate response
\end{enumerate}

The key innovation is pipelined processing and efficient DPU scheduling
on the AMD Kria KV260, which has limited memory and a single DPU but
powerful acceleration when properly leveraged.

\subsection{Detailed Design}\label{subsec:detailed-design}

Key system components:

\subsubsection{Hardware Platform}
\begin{itemize}
  \item \textbf{AMD Kria KV260 Development
    Board}~\cite{xilinx2022kv260} (see Figure~\ref{fig:kv260})
    \begin{itemize}
      \item System-on-Module (SoM) with programmable logic
      \item Quad-core ARM processor
      \item DPU synthesized onto FPGA fabric for accelerating neural network inference
      \item Four 1GB DDR4 memory banks
      \item Various I/O interfaces for camera input and system communication
    \end{itemize}
\end{itemize}

\subsubsection{Software Components}
\begin{enumerate}
  \item \textbf{U-Net Semantic Segmentation Algorithm} (see
    Figure~\ref{fig:unet})
    \begin{itemize}
      \item \textbf{Purpose:} Processes eye images to create
        pixel-level segmentation for pupil identification
      \item \textbf{Capabilities:} Achieves target accuracy while
        meeting performance requirements
      \item \textbf{Function:} Enables reliable eye tracking by
        producing high-quality segmentation maps
    \end{itemize}

  \item \textbf{Preprocessing Module}
    \begin{itemize}
      \item Handles image normalization, scaling, and initial filtering
      \item Prepares raw camera input for semantic segmentation
      \item Implemented as part of the pipeline before U-Net processing
    \end{itemize}

  \item \textbf{Blink Detection Algorithm}
    \begin{itemize}
      \item Lightweight neural network running alongside eye tracking
      \item Detects eye closure states to identify blinks
      \item Provides additional user intent information for the control system
    \end{itemize}

  \item \textbf{Thread Management System}~\cite{park2022thread}
    \begin{itemize}
      \item Coordinates execution across multiple threads
      \item Ensures proper synchronization between pipeline stages
      \item Manages resource allocation and conflict resolution
    \end{itemize}
\end{enumerate}

\subsubsection{Processing Pipeline}
Our system implements a four-stage processing pipeline:

\begin{enumerate}
  \item \textbf{Image Capture and Preprocessing}
    \begin{itemize}
      \item Raw camera input acquisition
      \item Image normalization and filtering
      \item Preparation for neural network processing
    \end{itemize}

  \item \textbf{Pipelined Semantic Segmentation}~\cite{zhao2023parallel}
    \begin{itemize}
      \item Processing pipeline optimization with overlapped CPU
        preprocessing/postprocessing stages and sequential DPU
        inference (U-Net model architecture unchanged)
      \item Efficient throughput of multiple frames through pipeline stages
      \item DPU scheduling to coordinate access among algorithms
    \end{itemize}

  \item \textbf{Feature Extraction and Analysis}
    \begin{itemize}
      \item Pupil identification and tracking
      \item Blink detection and classification
      \item Integration with assistive control systems
    \end{itemize}

  \item \textbf{Output Generation and Response}
    \begin{itemize}
      \item Real-time position calculation
      \item Medical distress detection
      \item Autonomous wheelchair control responses
    \end{itemize}
\end{enumerate}

\subsubsection{Memory Allocation Strategy}
\begin{itemize}
  \item \textbf{Distributed Memory Usage:} Utilize all four DDR4
    memory banks efficiently~\cite{chen2022memory}
  \item \textbf{Buffer Management:} Implement circular buffers for
    smooth data flow
  \item \textbf{Cache Optimization:} Minimize memory transfers
    between processing stages
  \item \textbf{Thread-Local Storage:} Allocate dedicated memory
    regions for each processing thread
\end{itemize}

\subsection{Functionality}\label{subsec:functionality}

\subsubsection{Initial Setup:}
\begin{itemize}
  \item System initialization and hardware configuration
  \item Neural network model loading and DPU programming
  \item Thread creation and synchronization primitive setup
  \item Memory allocation and buffer initialization
\end{itemize}

\subsubsection{Normal Operation:}
\begin{itemize}
  \item Continuous image capture at required frame rate
  \item Parallel processing through optimized pipeline
  \item Real-time eye tracking and position calculation
  \item Periodic data collection for auxiliary algorithms
\end{itemize}

\subsubsection{Response to Detected Issues:}
\begin{itemize}
  \item Automatic error detection and recovery mechanisms
  \item Graceful degradation under resource constraints
  \item User alert system for detected medical issues
  \item Safe positioning and emergency response protocols
\end{itemize}

\subsubsection{User Control Mode:}
\begin{itemize}
  \item Manual override capabilities for caregivers
  \item Adjustable sensitivity and response parameters
  \item System status monitoring and diagnostics
  \item Configuration interface for personalization
\end{itemize}

\subsection{Areas of Concern and Development}\label{subsec:areas-concern}

\begin{itemize}
  \item \textbf{Performance Optimization:} Achieving target
    throughput while maintaining accuracy
  \item \textbf{Resource Management:} Efficient DPU utilization
    across multiple algorithms
  \item \textbf{Thread Synchronization:} Preventing race conditions
    and deadlocks
  \item \textbf{Memory Efficiency:} Minimizing memory footprint and
    transfer overhead
  \item \textbf{Error Handling:} Robust recovery from system failures
  \item \textbf{Real-time Constraints:} Meeting strict timing requirements
\end{itemize}

\section{Technology Considerations}\label{sec:technology-considerations}

\subsection{Kria Board KV260}
The AMD Kria KV260 balances performance and power efficiency:

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{assets/kv260.png}
  \caption{AMD Kria KV260 Development Board featuring Zynq
  UltraScale+ MPSoC with DPU synthesized onto FPGA fabric via bitstream for AI acceleration.}~\label{fig:kv260}
\end{figure}

\begin{itemize}
  \item \textbf{Processing Power:} Quad-core ARM Cortex-A53 with 1.5
    GHz clock speed
  \item \textbf{AI Acceleration:} DPU synthesized onto FPGA fabric via bitstream for neural network inference
  \item \textbf{Memory System:} 4GB DDR4 memory with four banks for
    parallel access
  \item \textbf{Connectivity:} Multiple I/O interfaces for camera
    integration and system communication
  \item \textbf{Power Efficiency:} Low power consumption suitable for
    mobile applications
\end{itemize}

\subsubsection{DPU Architecture Options}
The KV260's FPGA fabric offers DPU configuration flexibility. The maximum
single-core configuration is B4096 at 300MHz~\cite{amd_ug1354_kv260,amd_kv260_b4096_forum}.
Alternative configurations:

\begin{itemize}
  \item \textbf{Single Large Core:} B4096 DPU at 300MHz (current implementation)
  \item \textbf{Dual Smaller Cores:} Two B1024 or B512 DPUs enabling
    true multi-core parallel processing
\end{itemize}

While dual smaller DPUs could benefit our workload with multiple concurrent
algorithms, the client specified maintaining single B4096 configuration,
necessitating software-level optimization and efficient resource scheduling
rather than hardware parallelism.

\subsection{U-Net Semantic Segmentation Algorithm}
The U-Net architecture~\cite{ronneberger2015} is particularly
well-suited for our eye tracking application:

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{assets/unet.png}
  \caption{U-Net encoder-decoder architecture with skip connections
    for semantic segmentation. The architecture features contracting
    path (left) for context capture and expansive path (right) for
  precise localization.}~\label{fig:unet}
\end{figure}

\begin{itemize}
  \item \textbf{Encoder-Decoder Structure:} Provides both high-level
    context and detailed localization
  \item \textbf{Skip Connections:} Preserves fine-grained spatial
    information crucial for pupil detection
  \item \textbf{Proven Performance:} Demonstrated required accuracy
    in baseline implementation
  \item \textbf{Modular Design:} Amenable to division across multiple
    processing cores
\end{itemize}

\subsection{Vitis-AI and Vitis-Runtime}
Xilinx Vitis-AI~\cite{amd2023vitis} and ONNX Runtime~\cite{onnxruntime}
provide essential optimization tools:

\begin{itemize}
  \item \textbf{Model Optimization:} Quantization and pruning
    capabilities for performance improvement
  \item \textbf{DPU Integration:} Seamless interface with hardware
    acceleration resources
  \item \textbf{Performance Profiling:} Detailed analysis tools for
    bottleneck identification
  \item \textbf{Memory Management:} Efficient buffer allocation and
    transfer optimization
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/xilinx-vart-stack.png}
  \caption{Xilinx Vitis AI Runtime (VART) stack architecture showing
    the layered approach from application level through runtime
  libraries to hardware acceleration.}~\label{fig:vart-stack}
\end{figure}

The VART stack (Figure~\ref{fig:vart-stack}) shows the software stack from
application layer to DPU\@. This layered approach enables efficient inference
through optimized runtime libraries and hardware abstraction.

\subsection{Alternative Technologies Considered}
Alternative approaches evaluated:

\begin{itemize}
  \item \textbf{Cloud-Based Processing:} Rejected due to latency and
    connectivity requirements
  \item \textbf{Hardware Upgrades:} Considered but one of constraints 
  of our project imposed by our client was our hardware
  \item \textbf{Model Simplification:} Would compromise accuracy
    below medical requirements
  \item \textbf{Alternative Neural Networks:} U-Net provides optimal
    balance of accuracy and performance
\end{itemize}

\section{Design Analysis}\label{sec:design-analysis}

\subsection{Current Implementation Status:}
Benchmarking evaluated single-model and split-model implementations.
Single model: 529.39 ms latency with 98.8\% IoU accuracy. Split model
(4 segments): 784.29 ms latency, revealing 48.2\% overhead from thread
synchronization and data transfer. Results inform strategies focusing
on pipeline parallelism rather than simple algorithm division.

\subsection{Implementation Challenges:}
\begin{itemize}
  \item Single DPU constraint requiring scheduling among multiple algorithms
  \item Memory bandwidth constraints affecting data flow between
    processing stages
  \item Thread synchronization overhead in pipelined processing
  \item Balancing DPU access time allocation between multiple algorithms
\end{itemize}

\subsection{Future Implementation Plans:}
\begin{itemize}
  \item Complete pipelined architecture implementation with optimized
    CPU processing stages
  \item Implement comprehensive DPU scheduling system
  \item Optimize memory access patterns and buffer management
  \item Develop robust testing and validation framework
\end{itemize}
