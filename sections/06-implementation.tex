\chapter{Implementation}\label{chap:implementation}

\section{Current Implementation Status}\label{sec:current-status}

Comprehensive performance evaluation of single-model and split-model architectures completed on the AMD Kria KV260 using Vitis-AI~\cite{amd2023vitis}, Git version control, and Docker-based build environment.

\subsection{Performance Results}\label{subsec:performance-results}

We have conducted extensive benchmarking of both single-model and split-model implementations to evaluate the trade-offs between different parallelization strategies.

\subsubsection{Single Model Performance}\label{subsubsec:single-model-performance}

The baseline U-Net implementation running as a single unified model demonstrates the following performance characteristics:

\begin{itemize}
\item \textbf{Mean Total Latency}: 529.39 ms ($\pm$ 0.26 ms)
\item \textbf{Mean DPU Time}: 474.32 ms ($\pm$ 0.05 ms)
\item \textbf{Mean Preprocess}: 27.43 ms ($\pm$ 0.23 ms)
\item \textbf{Mean Postprocess}: 27.64 ms ($\pm$ 0.08 ms)
\item \textbf{Memory Usage}: 45.00 MB
\end{itemize}

The single model implementation provides a stable baseline with low variance in processing times, indicating consistent performance across frames. The dominant processing time is spent in DPU execution (89.6\% of total latency), with preprocessing and postprocessing contributing approximately 10\% of overall latency.

\subsubsection{Split Model Performance (4 Segments)}\label{subsubsec:split-model-performance}

The parallelized implementation dividing the U-Net algorithm into four segments demonstrates the following characteristics:

\begin{itemize}
\item \textbf{Mean Total Latency}: 784.29 ms ($\pm$ 3.39 ms)
\item \textbf{Mean DPU Time}: 729.00 ms ($\pm$ 3.38 ms)
\item \textbf{Mean Preprocess}: 27.64 ms ($\pm$ 0.18 ms)
\item \textbf{Mean Postprocess}: 27.64 ms ($\pm$ 0.01 ms)
\end{itemize}

The split model approach incurs additional overhead from thread coordination and data transfer between segments, resulting in 48.2\% higher total latency compared to the single model. The increased variance ($\pm$ 3.39 ms vs. $\pm$ 0.26 ms) indicates synchronization overhead and resource contention between parallel threads.

\subsection{Performance Analysis}\label{subsec:performance-analysis}

\begin{itemize}
\item \textbf{Parallelization Overhead}: The split model demonstrates that naive algorithm division introduces significant overhead (255 ms increase) due to thread synchronization and inter-segment data transfer
\item \textbf{Memory Efficiency}: Single model maintains a lower memory footprint (45 MB) when compared to split model inference (111.01 MB). This increase in memory footprint is caused by the intermediate storage of tensors needed as inputs for further UNet segments.
\end{itemize}

\subsection{Algorithm Analysis and Scheduling}\label{subsec:algorithm-analysis}

U-Net algorithm analyzed for pipelined optimization~\cite{zhao2023parallel}, achieving 98.8\% IoU accuracy~\cite{wang2021}. Sequential DPU scheduling (round-robin) implemented with multi-threading framework~\cite{park2022thread}, optimized memory allocation~\cite{chen2022memory}, and inter-thread synchronization for pipeline coordination.

\section{Model Training}\label{sec:model-training}

\subsection{Training Infrastructure}\label{subsec:training-infrastructure}

GPU-optimized PyTorch training~\cite{pytorch2023} with batched operations, in-GPU metrics calculation~\cite{elvinger2025gpu}, and minimized CPU-GPU transfers reduced training time from 12 hours to 50 minutes (15x speedup) while improving IoU accuracy from 98.8\% to 98.9\%. The speedup results from eliminating CPU-GPU round trips and maintaining computation within GPU kernels.

\subsection{Experiment Tracking and Dataset}\label{subsec:experiment-tracking}

MLflow~\cite{mlflow2023} tracks hyperparameters, training metrics, model artifacts, and performance profiling for reproducibility. GPU metrics computed device-side~\cite{elvinger2025gpu} enable fine-grained profiling and accurate resource measurement. Dataset uses 80/20 train/validation split with GPU-based real-time augmentation.

\section{Implementation Challenges}\label{subsec:implementation-challenges}

\subsection{Technical Challenges}\label{subsec:technical-challenges}

Key challenges include 4GB memory constraints, thread synchronization without deadlocks, efficient single-DPU scheduling balancing multiple algorithms, and achieving target pipelined throughput.

\section{Future Implementation Plans}\label{subsec:future-implementation}

Short-term (4 weeks): Complete pipelined U-Net implementation, optimize DPU scheduling, implement testing framework, validate accuracy. Medium-term (8 weeks): Achieve target performance, complete integration testing, optimize memory/buffers, implement error handling. Long-term (12 weeks): Full system validation, optimization, documentation, user acceptance testing.

\section{Methodology and Architecture}\label{subsec:implementation-methodology}

Agile methodology with 2-week sprints, Git workflow with code review, and automated testing integration. Development uses Vitis-AI~\cite{amd2023vitis}, Docker, GCC/G++, Vitis AI Profiler, and custom logging. Key metrics: >60 FPS throughput, 99.8\% IoU accuracy, <16.6ms latency. Modular architecture with clear separation of concerns, well-defined APIs, and thread management using mutexes/semaphores~\cite{park2022thread} for coordination and deadlock prevention.
